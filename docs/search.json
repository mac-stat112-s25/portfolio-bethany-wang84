[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "COMP/STAT112 Notebook",
    "section": "",
    "text": "Welcome\nWelcome to my online portfolio for COMP/STAT112 course taken at Macalester College. Please, use the side bar on the left for navigation.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "bw/bw-uni.html",
    "href": "bw/bw-uni.html",
    "title": "\n1  Univariate Viz\n",
    "section": "",
    "text": "Use this file to generate a professional looking univariate visualization. The visualization will not perfect the first time but you are expected to improve on it throughout the semester especially after covering advanced topics such as effective viz.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Read Palmer Penguins data from GitHub\ndf &lt;- read_csv(\n  \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv\"\n)\n\nRows: 344 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): species, island, sex\ndbl (5): bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, year\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Quick look\nglimpse(df)\n\nRows: 344\nColumns: 8\n$ species           &lt;chr&gt; \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"A…\n$ island            &lt;chr&gt; \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen\", …\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;dbl&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;dbl&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;chr&gt; \"male\", \"female\", \"female\", NA, \"female\", \"male\", \"f…\n$ year              &lt;dbl&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\nggplot(df, aes(x = body_mass_g)) +\n  geom_histogram(aes(y = ..density..),\n                 bins  = 30,\n                 fill  = \"#2C3E50\",\n                 color = \"white\",\n                 alpha = 0.8) +\n  geom_density(size = 1, color = \"#E74C3C\") +\n  labs(\n    title    = \"Distribution of Penguin Body Mass\",\n    subtitle = \"Palmer Penguins dataset\",\n    x        = \"Body Mass (g)\",\n    y        = \"Density\"\n  ) +\n  theme_minimal(base_size = 14)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Univariate Viz</span>"
    ]
  },
  {
    "objectID": "bw/bw-bi.html",
    "href": "bw/bw-bi.html",
    "title": "\n2  Bivariate Viz\n",
    "section": "",
    "text": "Use this file to generate a professional looking bivariate visualization. The visualization will not perfect the first time but you are expected to improve it throughout the semester especially after covering advanced topics such as effective viz.\n\n# Load required packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# 1. Read Palmer Penguins data from GitHub\ndf &lt;- read_csv(\n  \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv\"\n)\n\nRows: 344 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): species, island, sex\ndbl (5): bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, year\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# 2. Filter to complete cases for the two variables we'll plot\ndf_clean &lt;- df %&gt;%\n  drop_na(bill_length_mm, bill_depth_mm)\n\n# 3. Bivariate scatterplot with linear trend line\nggplot(df_clean, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  \n  # points\n  geom_point(\n    size  = 2,\n    alpha = 0.7,\n    color = \"#1F77B4\"\n  ) +\n  \n  # add a linear fit (no confidence ribbon)\n  geom_smooth(\n    method   = \"lm\",\n    se       = FALSE,\n    linetype = \"dashed\",\n    color    = \"#FF7F0E\",\n    size     = 0.8\n  ) +\n  \n  # labels\n  labs(\n    title    = \"Bill Depth vs. Bill Length in Palmer Penguins\",\n    subtitle = \"Scatterplot with linear trend line\",\n    x        = \"Bill Length (mm)\",\n    y        = \"Bill Depth (mm)\"\n  ) +\n  \n  # clean, minimal theme\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title       = element_text(face = \"bold\"),\n    plot.subtitle    = element_text(size = 12, margin = margin(b = 10)),\n    axis.title       = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bivariate Viz</span>"
    ]
  },
  {
    "objectID": "bw/bw-tri.html",
    "href": "bw/bw-tri.html",
    "title": "\n3  Trivariate Viz\n",
    "section": "",
    "text": "Use this file to generate a professional looking trivariate visualization. The visualization will not perfect the first time but you are expected to improve on it throughout the semester especially after covering advanced topics such as effective viz.\n\n# Load needed packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# 1. Read Palmer Penguins data from GitHub\npenguins &lt;- read_csv(\n  \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv\"\n)\n\nRows: 344 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): species, island, sex\ndbl (5): bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, year\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# 2. Filter to complete cases for the three variables we'll plot\npenguins_clean &lt;- penguins %&gt;%\n  drop_na(bill_length_mm, bill_depth_mm, species)\n\n# 3. Trivariate scatterplot:\n#    – x: bill_length_mm\n#    – y: bill_depth_mm\n#    – color: species (third variable)\nggplot(penguins_clean,\n       aes(x = bill_length_mm,\n           y = bill_depth_mm,\n           color = species)) +\n  \n  # points\n  geom_point(size  = 3,\n             alpha = 0.8) +\n  \n  # labels\n  labs(\n    title    = \"Bill Depth vs. Bill Length by Penguin Species\",\n    subtitle = \"Palmer Penguins dataset (trivariate: length, depth, species)\",\n    x        = \"Bill Length (mm)\",\n    y        = \"Bill Depth (mm)\",\n    color    = \"Species\"\n  ) +\n  \n  # optional: use a colorblind-friendly palette\n  scale_color_brewer(palette = \"Set2\") +\n  \n  # clean, minimal theme\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title       = element_text(face = \"bold\"),\n    plot.subtitle    = element_text(size = 12, margin = margin(b = 8)),\n    axis.title       = element_text(face = \"bold\"),\n    legend.title     = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Trivariate Viz</span>"
    ]
  },
  {
    "objectID": "bw/bw-quad.html",
    "href": "bw/bw-quad.html",
    "title": "\n4  Quadvariate Viz\n",
    "section": "",
    "text": "Use this file to generate a professional looking quadvariate visualization. The visualization will not perfect the first time but you are expected to improve on it throughout the semester especially after covering advanced topics such as effective viz.\n\n# Load required packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# 1. Read Palmer Penguins data from GitHub\npenguins &lt;- read_csv(\n  \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv\"\n)\n\nRows: 344 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): species, island, sex\ndbl (5): bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, year\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# 2. Filter to complete cases for our four variables:\n#    – x: bill_length_mm (numeric)\n#    – y: bill_depth_mm  (numeric)\n#    – color: species    (categorical)\n#    – size: body_mass_g (numeric)\npenguins_clean &lt;- penguins %&gt;%\n  drop_na(bill_length_mm, bill_depth_mm, species, body_mass_g)\n\n# 3. Quadvariate scatterplot:\n#    • x-axis:  bill_length_mm\n#    • y-axis:  bill_depth_mm\n#    • color:   species\n#    • size:    body_mass_g\nggplot(penguins_clean,\n       aes(x = bill_length_mm,\n           y = bill_depth_mm,\n           color = species,\n           size = body_mass_g)) +\n  \n  # points with transparency\n  geom_point(alpha = 0.75) +\n  \n  # scale the size range so circles aren’t too large\n  scale_size_continuous(range = c(1, 6), name = \"Body Mass (g)\") +\n  \n  # colorblind-friendly palette\n  scale_color_brewer(palette = \"Dark2\") +\n  \n  # labels\n  labs(\n    title    = \"Bill Depth vs. Bill Length with Species & Body Mass\",\n    subtitle = \"Quadvariate visualization from Palmer Penguins dataset\",\n    x        = \"Bill Length (mm)\",\n    y        = \"Bill Depth (mm)\",\n    color    = \"Species\"\n  ) +\n  \n  # clean, minimal theme\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title       = element_text(face = \"bold\"),\n    plot.subtitle    = element_text(size = 12, margin = margin(b = 8)),\n    axis.title       = element_text(face = \"bold\"),\n    legend.title     = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Quadvariate Viz</span>"
    ]
  },
  {
    "objectID": "bw/bw-spatial.html",
    "href": "bw/bw-spatial.html",
    "title": "\n5  Spatial Viz\n",
    "section": "",
    "text": "Use this file to generate a professional looking spatial visualization. The visualization will not perfect the first time but you are expected to improve on it throughout the semester especially after covering advanced topics such as effective viz.\n\n# 1. Load libraries\nlibrary(sf)\n\nLinking to GEOS 3.13.0, GDAL 3.8.5, PROJ 9.5.1; sf_use_s2() is TRUE\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# 2. Read the US-states GeoJSON directly as an sf\nus_states &lt;- read_sf(\n  \"https://raw.githubusercontent.com/PublicaMundi/MappingAPI/master/data/geojson/us-states.json\",\n  quiet = TRUE\n)\n\n# 3. (Optional) Reproject to an equal-area CRS for accurate area calcs\n#    EPSG:5070 is NAD83 / Conus Albers\nus_states &lt;- st_transform(us_states, crs = 5070)\n\n# 4. Compute area in km²\nus_states &lt;- us_states %&gt;%\n  mutate(\n    area_km2 = as.numeric(st_area(geometry)) / 1e6\n  )\n\n# 5. Plot a clean choropleth\nggplot(us_states) +\n  geom_sf(aes(fill = area_km2),\n          color = \"white\",\n          size  = 0.2) +\n  scale_fill_viridis_c(\n    option = \"plasma\",\n    name   = \"Area (km²)\"\n  ) +\n  labs(\n    title    = \"Area of US States (Equal-Area Projection)\",\n    subtitle = \"Choropleth from PublicaMundi GeoJSON\",\n    caption  = \"Source: github.com/PublicaMundi/MappingAPI\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title       = element_text(face = \"bold\"),\n    plot.subtitle    = element_text(margin = margin(b = 8)),\n    legend.title     = element_text(face = \"bold\"),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  )",
    "crumbs": [
      "Best Work",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Spatial Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-uni.html",
    "href": "ica/ica-uni.html",
    "title": "\n6  Univariate Viz\n",
    "section": "",
    "text": "# Import data\nhikes &lt;- read.csv(\"https://mac-stat.github.io/data/high_peaks.csv\")\n\n\n# Exercise 1:\n# Categorical (rating): observed categories, counts/proportions, relative prevalence,\n#                       logical order, clear axis labels.\n# Quantitative (elevation): range, center (mean/median), shape (skewness), spread, outliers.\n\n# Exercise 2: load ggplot tools\nlibrary(tidyverse)  # loads ggplot2 for visualization\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Exercise 3–5: Bar chart of difficulty ratings\nggplot(hikes, aes(x = rating)) +\n  geom_bar(color = \"orange\", fill = \"blue\") +\n  labs(x = \"Rating\", y = \"Number of hikes\") +\n  theme_minimal()\n\n\n\n\n\n\n# • Shows the three observed difficulty levels\n# • Counts per level reveal that “moderate” is most common\n\n# Exercise 6: Bar chart of elevation (ineffective for continuous data)\nggplot(hikes, aes(x = elevation)) +\n  geom_bar() +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n\n\n\n\n# • A bar for each unique elevation leads to clutter\n# • Fails to reveal overall distribution shape\n\n# Exercise 8–10: Histograms of elevation\n\n# Default bins\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram() +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n# • Rough view of distribution; bin boundaries may be arbitrary\n\n# White borders for clarity\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\") +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n# • Separates bins visually, improving readability\n\n# Blue fill\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\", fill = \"blue\") +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n# • Combines border and fill for better aesthetics\n\n# Oversmoothed (binwidth = 1000)\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\", binwidth = 1000) +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n\n\n\n\n# • Very wide bins hide detail; distribution too coarse\n\n# Noisy (binwidth = 5)\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\", binwidth = 5) +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n\n\n\n\n# • Very narrow bins create noisy, spiky plot\n\n# Balanced (binwidth = 200)\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\", fill = \"blue\", binwidth = 200) +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n\n\n\n\n# • Binwidth reveals center, spread, skew, and outliers effectively\n\n# Exercise 11: Density plot of elevation\nggplot(hikes, aes(x = elevation)) +\n  geom_density(color = \"blue\", fill = \"orange\", alpha = 0.5) +\n  labs(x = \"Elevation (feet)\", y = \"Density\")\n\n\n\n\n\n\n# • Smooth distribution (area = 1), shows typical elevation, variability, skewness\n\nUse this file for practice with the univariate viz in-class activity. Refer to the class website for details.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Univariate Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-bi.html",
    "href": "ica/ica-bi.html",
    "title": "\n7  Bivariate Viz\n",
    "section": "",
    "text": "# Load data\nelections &lt;- read.csv(\"https://mac-stat.github.io/data/election_2020_county.csv\")\n\n# Check it out\nhead(elections)\n\n  state_name state_abbr historical    county_name county_fips total_votes_20\n1    Alabama         AL        red Autauga County        1001          27770\n2    Alabama         AL        red Baldwin County        1003         109679\n3    Alabama         AL        red Barbour County        1005          10518\n4    Alabama         AL        red    Bibb County        1007           9595\n5    Alabama         AL        red  Blount County        1009          27588\n6    Alabama         AL        red Bullock County        1011           4613\n  repub_pct_20 dem_pct_20 winner_20 total_votes_16 repub_pct_16 dem_pct_16\n1        71.44      27.02     repub          24661        73.44      23.96\n2        76.17      22.41     repub          94090        77.35      19.57\n3        53.45      45.79     repub          10390        52.27      46.66\n4        78.43      20.70     repub           8748        76.97      21.42\n5        89.57       9.57     repub          25384        89.85       8.47\n6        24.84      74.70       dem           4701        24.23      75.09\n  winner_16 total_votes_12 repub_pct_12 dem_pct_12 winner_12 total_population\n1     repub          23909        72.63      26.58     repub            54907\n2     repub          84988        77.39      21.57     repub           187114\n3     repub          11459        48.34      51.25       dem            27321\n4     repub           8391        73.07      26.22     repub            22754\n5     repub          23980        86.49      12.35     repub            57623\n6       dem           5318        23.51      76.31       dem            10746\n  percent_white percent_black percent_asian percent_hispanic per_capita_income\n1            76            18             1                2             24571\n2            83             9             1                4             26766\n3            46            46             0                5             16829\n4            75            22             0                2             17427\n5            88             1             0                8             20730\n6            22            71             0                6             18628\n  median_rent median_age\n1         668       37.5\n2         693       41.5\n3         382       38.3\n4         351       39.4\n5         403       39.6\n6         276       39.6\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Exercise 0a: Percent of counties won by Republicans in 2020\nprop_repub_2020 &lt;- mean(elections$winner_20 == \"repub\")\nprop_repub_2020  # ~ proportion of counties won by the GOP\n\n[1] 0.8275973\n\n# Plot the 2020 winner distribution\nggplot(elections, aes(x = winner_20)) +\n  geom_bar(fill = c(\"red\", \"blue\")) +\n  labs(x = \"2020 Winner\", y = \"Number of Counties\")\n\n\n\n\n\n\n# • Shows counts of counties won by each party\n# • Follow-up: which regions drove this split?\n\n# Exercise 0b: Distribution of Republican vote share in 2020\nggplot(elections, aes(x = repub_pct_20)) +\n  geom_histogram(color = \"white\", bins = 30) +\n  labs(x = \"Republican Vote % (2020)\", y = \"County Count\")\n\n\n\n\n\n\n# • Slight left skew: more counties with &gt;50% Republican support\n# • Follow-up: which demographics correlate with high GOP support?\n\n# Exercise 1: Scatter of repub_pct_20 vs repub_pct_16\nggplot(elections, aes(x = repub_pct_16, y = repub_pct_20)) +\n  geom_point() +\n  labs(x = \"GOP % in 2016\", y = \"GOP % in 2020\")\n\n\n\n\n\n\n# • Data frame + mapping; geom_point draws the dots\n\n# Exercise 2: Customize scatter glyphs and labels\nggplot(elections, aes(x = repub_pct_16, y = repub_pct_20)) +\n  geom_point(shape = 3, color = \"orange\")\n\n\n\n\n\n\n# • shape=3 changes the symbol to a plus; color sets point color\n\nggplot(elections, aes(x = repub_pct_16, y = repub_pct_20, label = state_abbr)) +\n  geom_text(size = 2)\n\n\n\n\n\n\n# • geom_text labels each point with its state abbreviation\n\n# Exercise 3: Relationship summary\n# • Strong, positive linear trend\n# • Counties with higher 2016 GOP % tend to have higher 2020 GOP %\n# • Outliers often in swing states (e.g., some WI/MI counties deviated)\n\n# Exercise 4a: Smooth trend only\nggplot(elections, aes(x = repub_pct_16, y = repub_pct_20)) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n# • Local smoothing line without points\n\n# Exercise 4b: Linear model fit\nggplot(elections, aes(x = repub_pct_16, y = repub_pct_20)) +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n# • Best-fit straight line (linear regression)\n\n# Exercise 5: GOP % vs median_rent and median_age\nggplot(elections, aes(x = median_rent, y = repub_pct_20)) +\n  geom_point() + geom_smooth(method = \"lm\") +\n  labs(x = \"Median Rent\", y = \"GOP % (2020)\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n# • Moderately negative slope: higher rent → slightly less GOP support\n\nggplot(elections, aes(x = median_age, y = repub_pct_20)) +\n  geom_point() + geom_smooth(method = \"lm\") +\n  labs(x = \"Median Age\", y = \"GOP % (2020)\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n# • Weak positive slope: older counties → slightly more GOP support\n# • median_age shows a clearer trend (steeper slope) than median_rent\n\n# Exercise 6: GOP % vs historical (scatter)\nggplot(elections, aes(x = historical, y = repub_pct_20)) +\n  geom_point()\n\n\n\n\n\n\n# • Overplotting makes it hard to compare distributions by category\n\n# Exercise 7: Violin and boxplots by historical category\nggplot(elections, aes(x = historical, y = repub_pct_20)) +\n  geom_violin() +\n  labs(x = \"State Trend\", y = \"GOP % (2020)\")\n\n\n\n\n\n\n# • Violin shows full density shape per category\n\nggplot(elections, aes(x = historical, y = repub_pct_20)) +\n  geom_boxplot() +\n  labs(x = \"State Trend\", y = \"GOP % (2020)\")\n\n\n\n\n\n\n# • Boxplot summarizes median, IQR, range; red &gt; purple &gt; blue in median GOP %\n\n# Exercise 8: Faceted density plots by historical\nggplot(elections, aes(x = repub_pct_20, fill = historical)) +\n  geom_density(alpha = 0.5) +\n  scale_fill_manual(values = c(\"red\",\"purple\",\"blue\")) +\n  facet_wrap(~ historical) +\n  labs(x = \"GOP % (2020)\", y = \"Density\")\n\n\n\n\n\n\n# • Faceting separates categories; alpha = 0.5 adds transparency\n\n# Exercise 9: Density with manual colors and transparency\nggplot(elections, aes(x = repub_pct_20, fill = historical)) +\n  geom_density(alpha = 0.5) +\n  scale_fill_manual(values = c(\"red\",\"purple\",\"blue\"))\n\n\n\n\n\n\n# • scale_fill_manual assigns specific colors\n\n# Exercise 10: Reflection (no code)\n# • Favorite: boxplots for quick comparison of medians and IQRs.\n# • Pro density vs box: shows full distribution shape.\n# • Con density vs box: less precise quantile information.\n\n# Exercise 11: Categorical vs categorical (bar plots)\nggplot(elections, aes(x = historical)) + geom_bar()\n\n\n\n\n\n\n# • Counts of counties by state trend\n\nggplot(elections, aes(x = winner_20)) + geom_bar()\n\n\n\n\n\n\n# • Counts of GOP vs Dem county wins\n\n# Exercise 12: Four bar-plot types\n# Stacked\nggplot(elections, aes(x = historical, fill = winner_20)) + geom_bar()\n\n\n\n\n\n\n# Faceted\nggplot(elections, aes(x = winner_20)) + geom_bar() + facet_wrap(~ historical)\n\n\n\n\n\n\n# Side-by-side\nggplot(elections, aes(x = historical, fill = winner_20)) + geom_bar(position = \"dodge\")\n\n\n\n\n\n\n# Proportional\nggplot(elections, aes(x = historical, fill = winner_20)) + geom_bar(position = \"fill\")\n\n\n\n\n\n\n# • Pro of proportional: compares proportions directly.\n# • Con: loses absolute count information.\n\nUse this file for practice with the bivariate viz in-class activity. Refer to the class website for details.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bivariate Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-multi.html",
    "href": "ica/ica-multi.html",
    "title": "\n8  Mulivariate Viz\n",
    "section": "",
    "text": "# Import and check out data\neducation &lt;- read.csv(\"https://mac-stat.github.io/data/sat.csv\")\nhead(education)\n\n       State expend ratio salary frac verbal math  sat  fracCat\n1    Alabama  4.405  17.2 31.144    8    491  538 1029   (0,15]\n2     Alaska  8.963  17.6 47.951   47    445  489  934 (45,100]\n3    Arizona  4.778  19.3 32.175   27    448  496  944  (15,45]\n4   Arkansas  4.459  17.1 28.934    6    482  523 1005   (0,15]\n5 California  4.992  24.0 41.078   45    417  485  902  (15,45]\n6   Colorado  5.443  18.4 34.571   29    462  518  980  (15,45]\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Exercise 1a: Plot average SAT scores by state\nggplot(education, aes(x = reorder(State, sat), y = sat)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(x = \"State\", y = \"Average SAT\")\n\n\n\n\n\n\n# • Range: ~900 to ~1100\n# • Typical: most states around ~1000\n# • Shape: a few high‐scoring outliers, slight skew\n# • Non‐normal: extreme values (e.g. small states or selective testers)\n\n# Exercise 2a: SAT vs per‐pupil spending\nggplot(education, aes(x = expend, y = sat)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Expenditure per Pupil (thousands $)\", y = \"Average SAT\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n# Trend: slight negative slope—higher spending ↔ lower SAT overall\n\n# Exercise 2a: SAT vs teacher salary\nggplot(education, aes(x = salary, y = sat)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Average Teacher Salary (thousands $)\", y = \"Average SAT\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n# Trend: flat-to-slight negative—no clear positive link between salary and SAT\n\n# Exercise 3: Trivariate viz (sat ~ expend colored by salary)\nggplot(education, aes(x = expend, y = sat, color = salary)) +\n  geom_point(size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(color = \"Salary (k$)\", x = \"Spending (k$)\", y = \"Average SAT\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: The following aesthetics were dropped during statistical transformation:\ncolour.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\n\n# • Color gradient shows salary levels\n# • Both higher spending and higher salary states tend to cluster differently,\n#   but overall relationships remain weak/negative\n\n# Exercise 4: Discretize spending into 2 groups\nggplot(education, aes(x = salary, y = sat, color = cut(expend, 2))) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(color = \"Spending Group\", x = \"Salary (k$)\", y = \"Average SAT\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n# • cut(...,2) splits into low/high spending\n# • If cut(...,3), you get low/medium/high—more nuance\n# • Within each spending group, SAT and salary trend positive or flat\n\n# Exercise 5a: Univariate viz of fracCat\nggplot(education, aes(x = fracCat)) +\n  geom_bar(fill = \"lightgreen\") +\n  labs(x = \"SAT Participation Category\", y = \"Number of States\")\n\n\n\n\n\n\n# • Shows counts: most states medium (15–45%), fewer low (&lt;15%) or high (&gt;45%)\n\n# Exercise 5b: SAT vs fracCat (boxplot & violin)\nggplot(education, aes(x = fracCat, y = sat)) +\n  geom_boxplot(fill = \"orange\") +\n  labs(x = \"Participation Category\", y = \"Average SAT\")\n\n\n\n\n\n\n# • Low participation → highest median SAT\n# • High participation → lowest median SAT\n\nggplot(education, aes(x = fracCat, y = sat)) +\n  geom_violin(trim = FALSE, fill = \"purple\", alpha = 0.6) +\n  labs(x = \"Participation Category\", y = \"Average SAT\")\n\n\n\n\n\n\n# • Violin confirms boxplot pattern and shows full distribution shape\n\n# Exercise 5c: Trivariate viz (sat ~ expend colored by fracCat)\nggplot(education, aes(x = expend, y = sat, color = fracCat)) +\n  geom_point(size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(color = \"Participation\", x = \"Spending (k$)\", y = \"Average SAT\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n# • Within each fracCat group, spending and SAT correlate positively\n# • Overall negative trend is due to mixing groups with different fracCat\n\n# Exercise 5d: Simpson’s Paradox explanation\n# • Low‐participation states (small test‐taker fraction) have high SAT but low spending.\n# • High‐participation states have lower SAT but higher spending.\n# • Aggregating creates a misleading negative overall correlation,\n#   whereas controlling for fracCat reveals the true within‐group positive relationships.\n\nUse this file for practice with the mulivariate viz in-class activity. Refer to the class website for details.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Mulivariate Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-spatial.html",
    "href": "ica/ica-spatial.html",
    "title": "9  Spatial Viz",
    "section": "",
    "text": "Use this file for practice with the spatial viz in-class activity. Refer to the class website for details.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Spatial Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-effective.html",
    "href": "ica/ica-effective.html",
    "title": "\n10  Effective Viz\n",
    "section": "",
    "text": "# Load tidyverse package for plotting and wrangling\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Import the data\nweather &lt;- read.csv(\"https://mac-stat.github.io/data/weather_3_locations.csv\") |&gt; \n  mutate(date = as.Date(date))\n\n\n# Exercise 1a & 1b: Professionalism\n#| fig-cap: \"Scatterplot of 9 am vs 3 pm temperatures in three Australian locations.\"\nggplot(weather, aes(x = temp9am, y = temp3pm, color = location)) + \n  geom_point() + \n  labs(\n    x     = \"Morning Temperature (°C)\",      # A: short label with units\n    y     = \"Afternoon Temperature (°C)\",    # B: short label with units\n    title = \"Daily Temperature Comparison\",  # C: &lt;10-word descriptive title\n    color = \"Location\"                       # D: legend title\n  )\n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n# Answers for Part a:\n# A = \"Morning Temperature (°C)\"\n# B = \"Afternoon Temperature (°C)\"\n# C = \"Daily Temperature Comparison\"\n# D = \"Location\"\n\n# Exercise 2: Accessibility\n#| fig-cap: \"Density plots of 3 pm temperatures in three Australian locations.\"\n#| fig-alt: \"Overlaid density curves of 3 pm temperatures in Hobart, Uluru, and Wollongong, showing Hobart’s temps cluster around 15–20 °C, Uluru’s cluster around 25–30 °C, and Wollongong in between. Data from mac-stat.github.io.\"\nggplot(weather, aes(x = temp3pm, fill = location)) + \n  geom_density(alpha = 0.5) + \n  labs(\n    x    = \"3 pm Temperature (°C)\",\n    fill = \"Location\"\n  )\n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n# Accessibility notes:\n# - fig-alt describes plot type, locations, key takeaway, and data source.\n# - Test colors under color-blind simulators; consider scale_fill_viridis_d() for a color-blind–safe palette.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Effective Viz</span>"
    ]
  },
  {
    "objectID": "ica/ica-wrangling.html",
    "href": "ica/ica-wrangling.html",
    "title": "\n11  Wrangling\n",
    "section": "",
    "text": "# Load tidyverse & data\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nelections &lt;- read.csv(\"https://mac-stat.github.io/data/election_2020_county.csv\")\n# Get a background map\nlibrary(socviz)\ndata(county_map)\n\n# Make a choropleth map\nlibrary(RColorBrewer)  # For the color scale\nlibrary(ggthemes) # For theme_map\nelections |&gt; \n  mutate(county_fips = as.character(county_fips)) |&gt; \n  mutate(county_fips = \n           ifelse(nchar(county_fips) == 4, paste0(\"0\", county_fips), county_fips)) |&gt; \n  ggplot(aes(map_id = county_fips, fill = cut(repub_pct_20, breaks = seq(0, 100, by = 10)))) +\n    geom_map(map = county_map) +\n    scale_fill_manual(values = rev(brewer.pal(10, \"RdBu\")), name = \"% Republican\") +\n    expand_limits(x = county_map$long, y = county_map$lat)  + \n    theme_map() +\n    theme(legend.position = \"right\") + \n    coord_equal()\n\n\n\n\n\n\n\n\n# Load tidyverse for data wrangling and plotting\nlibrary(tidyverse)\n\n# Exercise 1: select Practice\n# Create elections_small with only the specified columns\nelections_small &lt;- elections %&gt;%\n  select(state_name, county_name, total_votes_20, repub_pct_20, dem_pct_20,\n         total_votes_16, dem_pct_16)\nhead(elections_small)  # confirm it worked\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16\n1          24661      23.96\n2          94090      19.57\n3          10390      46.66\n4           8748      21.42\n5          25384       8.47\n6           4701      75.09\n\n# Exercise 2: filter Demo\n# Keep only counties in Hawaii\nelections_small %&gt;% \n  filter(state_name == \"Hawaii\")\n\n  state_name     county_name total_votes_20 repub_pct_20 dem_pct_20\n1     Hawaii   Hawaii County          87814        30.63      66.88\n2     Hawaii Honolulu County         382114        35.66      62.51\n3     Hawaii    Kauai County          33497        34.58      63.36\n4     Hawaii     Maui County          71044        31.14      66.59\n  total_votes_16 dem_pct_16\n1          64865      63.61\n2         285683      61.48\n3          26335      62.49\n4          51942      64.45\n\n# Keep only counties in Hawaii or Delaware\nelections_small %&gt;% \n  filter(state_name %in% c(\"Hawaii\", \"Delaware\"))\n\n  state_name       county_name total_votes_20 repub_pct_20 dem_pct_20\n1   Delaware       Kent County          87025        47.12      51.19\n2   Delaware New Castle County         287633        30.72      67.81\n3   Delaware     Sussex County         129352        55.07      43.82\n4     Hawaii     Hawaii County          87814        30.63      66.88\n5     Hawaii   Honolulu County         382114        35.66      62.51\n6     Hawaii      Kauai County          33497        34.58      63.36\n7     Hawaii       Maui County          71044        31.14      66.59\n  total_votes_16 dem_pct_16\n1          74253      44.91\n2         261468      62.30\n3         105814      37.17\n4          64865      63.61\n5         285683      61.48\n6          26335      62.49\n7          51942      64.45\n\n# Counties where GOP got &gt; 93.97% in 2020\nelections_small %&gt;% \n  filter(repub_pct_20 &gt; 93.97)\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1      Texas  Borden County            416        95.43       3.85\n2      Texas    King County            159        94.97       5.03\n3      Texas Roberts County            550        96.18       3.09\n  total_votes_16 dem_pct_16\n1            365       8.49\n2            159       3.14\n3            550       3.64\n\n# Counties where GOP got ≥ 93.97% in 2020 (one more than above)\nelections_small %&gt;% \n  filter(repub_pct_20 &gt;= 93.97)\n\n  state_name     county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Montana Garfield County            813        93.97       5.04\n2      Texas   Borden County            416        95.43       3.85\n3      Texas     King County            159        94.97       5.03\n4      Texas  Roberts County            550        96.18       3.09\n  total_votes_16 dem_pct_16\n1            715       4.76\n2            365       8.49\n3            159       3.14\n4            550       3.64\n\n# Texas counties where Dems &gt; 65% in 2020, Method 1 (two filters)\nelections_small %&gt;% \n  filter(state_name == \"Texas\") %&gt;% \n  filter(dem_pct_20 &gt; 65)\n\n  state_name     county_name total_votes_20 repub_pct_20 dem_pct_20\n1      Texas  El Paso County         267215        31.56      66.66\n2      Texas Presidio County           2217        32.52      65.99\n3      Texas   Travis County         610349        26.43      71.41\n4      Texas   Zavala County           4379        34.03      65.40\n  total_votes_16 dem_pct_16\n1         210458      69.14\n2           2203      66.18\n3         462511      66.26\n4           3390      77.67\n\n# Texas counties where Dems &gt; 65% in 2020, Method 2 (one filter)\nelections_small %&gt;% \n  filter(state_name == \"Texas\", dem_pct_20 &gt; 65)\n\n  state_name     county_name total_votes_20 repub_pct_20 dem_pct_20\n1      Texas  El Paso County         267215        31.56      66.66\n2      Texas Presidio County           2217        32.52      65.99\n3      Texas   Travis County         610349        26.43      71.41\n4      Texas   Zavala County           4379        34.03      65.40\n  total_votes_16 dem_pct_16\n1         210458      69.14\n2           2203      66.18\n3         462511      66.26\n4           3390      77.67\n\n# Exercise 3: arrange Demo\n# Counties sorted lowest to highest GOP % in 2020\nelections_small %&gt;% \n  arrange(repub_pct_20) %&gt;% \n  head()\n\n            state_name            county_name total_votes_20 repub_pct_20\n1 District of Columbia   District of Columbia         344356         5.40\n2             Maryland Prince George's County         424855         8.73\n3             Maryland         Baltimore city         237461        10.69\n4             Virginia        Petersburg city          14118        11.22\n5             New York        New York County         694904        12.26\n6           California   San Francisco County         443458        12.72\n  dem_pct_20 total_votes_16 dem_pct_16\n1      92.15         280272      92.85\n2      89.26         351091      89.33\n3      87.28         208980      85.44\n4      87.75          13717      87.52\n5      86.78         591368      87.17\n6      85.27         365295      85.53\n\n# Counties sorted highest to lowest GOP % in 2020\nelections_small %&gt;% \n  arrange(desc(repub_pct_20)) %&gt;% \n  head()\n\n  state_name      county_name total_votes_20 repub_pct_20 dem_pct_20\n1      Texas   Roberts County            550        96.18       3.09\n2      Texas    Borden County            416        95.43       3.85\n3      Texas      King County            159        94.97       5.03\n4    Montana  Garfield County            813        93.97       5.04\n5      Texas Glasscock County            653        93.57       5.97\n6   Nebraska     Grant County            402        93.28       4.98\n  total_votes_16 dem_pct_16\n1            550       3.64\n2            365       8.49\n3            159       3.14\n4            715       4.76\n5            602       5.65\n6            394       5.08\n\n# Exercise 4: mutate Demo\n# a) Create diff_20 = repub_pct_20 − dem_pct_20\nelections_small %&gt;% \n  mutate(diff_20 = repub_pct_20 - dem_pct_20) %&gt;% \n  head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 diff_20\n1          24661      23.96   44.42\n2          94090      19.57   53.76\n3          10390      46.66    7.66\n4           8748      21.42   57.73\n5          25384       8.47   80.00\n6           4701      75.09  -49.86\n\n# b) Compute repub_votes_20 via rounding\nelections_small %&gt;% \n  mutate(repub_votes_20 = round(total_votes_20 * repub_pct_20 / 100)) %&gt;% \n  head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 repub_votes_20\n1          24661      23.96          19839\n2          94090      19.57          83542\n3          10390      46.66           5622\n4           8748      21.42           7525\n5          25384       8.47          24711\n6           4701      75.09           1146\n\n# c) Logical column: did GOP win in 2020?\nelections_small %&gt;% \n  mutate(repub_win_20 = repub_pct_20 &gt; dem_pct_20) %&gt;% \n  head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 repub_win_20\n1          24661      23.96         TRUE\n2          94090      19.57         TRUE\n3          10390      46.66         TRUE\n4           8748      21.42         TRUE\n5          25384       8.47         TRUE\n6           4701      75.09        FALSE\n\n# Part b: Define Dem change and indicator\n# Change in Dem support 2020 vs 2016\nelections_small %&gt;% \n  mutate(dem_change = dem_pct_20 - dem_pct_16) %&gt;% \n  head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 dem_change\n1          24661      23.96       3.06\n2          94090      19.57       2.84\n3          10390      46.66      -0.87\n4           8748      21.42      -0.72\n5          25384       8.47       1.10\n6           4701      75.09      -0.39\n\n# Was Dem support higher in 2020 than 2016?\nelections_small %&gt;% \n  mutate(dem_higher_20 = dem_pct_20 &gt; dem_pct_16) %&gt;% \n  head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 dem_higher_20\n1          24661      23.96          TRUE\n2          94090      19.57          TRUE\n3          10390      46.66         FALSE\n4           8748      21.42         FALSE\n5          25384       8.47          TRUE\n6           4701      75.09         FALSE\n\n# Exercise 5: Pipe Series\n# a) Wisconsin counties where GOP lost in 2020, sorted by turnout\nelections_small %&gt;% \n  filter(state_name == \"Wisconsin\", repub_pct_20 &lt; dem_pct_20) %&gt;% \n  arrange(desc(total_votes_20)) %&gt;% \n  head()\n\n  state_name       county_name total_votes_20 repub_pct_20 dem_pct_20\n1  Wisconsin  Milwaukee County         458971        29.27      69.13\n2  Wisconsin       Dane County         344791        22.85      75.46\n3  Wisconsin       Rock County          85360        43.51      54.66\n4  Wisconsin  La Crosse County          67884        42.25      55.75\n5  Wisconsin Eau Claire County          58275        43.49      54.26\n6  Wisconsin    Portage County          40603        47.53      50.31\n  total_votes_16 dem_pct_16\n1         434970      66.44\n2         304729      71.38\n3          75043      52.42\n4          62785      51.61\n5          54080      50.43\n6          38123      48.59\n\n# b) Swapping filter & arrange still yields same rows; filtering first reduces data before sorting.\n# c) Advantage: filtering early limits data volume for subsequent steps.\n# d) Delaware counties with GOP win indicator\nelections_small %&gt;% \n  filter(state_name == \"Delaware\") %&gt;% \n  mutate(repub_win_20 = repub_pct_20 &gt; dem_pct_20) %&gt;% \n  select(county_name, repub_pct_20, dem_pct_20, repub_win_20)\n\n        county_name repub_pct_20 dem_pct_20 repub_win_20\n1       Kent County        47.12      51.19        FALSE\n2 New Castle County        30.72      67.81        FALSE\n3     Sussex County        55.07      43.82         TRUE\n\n# e) If we select before mutate, repub_win_20 doesn't yet exist → error. Order matters when selecting new vars.\n\n# Exercise 6: DIY Pipe Series\n# a) MN counties and their 2020 Dem % from highest to lowest\nelections_small %&gt;% \n  filter(state_name == \"Minnesota\") %&gt;% \n  select(county_name, dem_pct_20) %&gt;% \n  arrange(desc(dem_pct_20))\n\n                county_name dem_pct_20\n1             Ramsey County      71.50\n2           Hennepin County      70.46\n3               Cook County      65.58\n4          St. Louis County      56.64\n5             Dakota County      55.73\n6            Olmsted County      54.16\n7         Washington County      53.46\n8         Blue Earth County      50.84\n9               Clay County      50.74\n10              Lake County      50.64\n11          Nicollet County      50.31\n12           Carlton County      49.58\n13            Winona County      49.07\n14              Rice County      48.76\n15          Mahnomen County      48.26\n16             Anoka County      47.79\n17          Beltrami County      47.24\n18            Carver County      46.37\n19             Mower County      46.00\n20             Scott County      45.52\n21           Houston County      42.42\n22           Goodhue County      41.23\n23          Freeborn County      40.96\n24            Norman County      40.80\n25            Itasca County      40.61\n26       Koochiching County      38.41\n27          Watonwan County      38.20\n28           Kittson County      38.12\n29           Stevens County      37.80\n30           Stearns County      37.58\n31          Fillmore County      37.48\n32            Steele County      37.47\n33         Kandiyohi County      36.12\n34            Aitkin County      35.98\n35              Lyon County      35.94\n36     Lac qui Parle County      35.79\n37           Wabasha County      35.78\n38             Grant County      35.58\n39          Traverse County      35.46\n40         Big Stone County      35.41\n41        Pennington County      35.29\n42              Pope County      35.27\n43              Polk County      34.88\n44              Cass County      34.68\n45            Wright County      34.49\n46           Hubbard County      34.42\n47             Swift County      34.35\n48         Crow Wing County      34.17\n49           Chisago County      34.15\n50            Becker County      33.96\n51              Pine County      33.87\n52          Le Sueur County      33.73\n53          Chippewa County      33.67\n54            Nobles County      33.65\n55            Waseca County      33.65\n56             Dodge County      33.47\n57        Otter Tail County      32.85\n58            Benton County      32.70\n59           Douglas County      32.56\n60             Brown County      32.48\n61         Sherburne County      32.48\n62         Faribault County      31.98\n63          Red Lake County      31.47\n64          Renville County      30.71\n65            McLeod County      30.64\n66   Yellow Medicine County      30.54\n67           Lincoln County      30.08\n68        Cottonwood County      30.03\n69           Kanabec County      30.02\n70            Martin County      30.02\n71           Jackson County      29.99\n72        Mille Lacs County      29.98\n73            Wilkin County      29.91\n74              Rock County      29.69\n75            Murray County      29.60\n76            Isanti County      29.45\n77            Sibley County      28.60\n78            Meeker County      28.58\n79           Redwood County      28.43\n80 Lake of the Woods County      27.87\n81        Clearwater County      26.76\n82         Pipestone County      26.44\n83            Wadena County      26.35\n84            Roseau County      25.98\n85          Marshall County      25.33\n86              Todd County      24.79\n87          Morrison County      22.33\n\n# b) mn_wi: change in Dem % for MN & WI, sorted by change ascending\nmn_wi &lt;- elections_small %&gt;% \n  filter(state_name %in% c(\"Minnesota\", \"Wisconsin\")) %&gt;% \n  mutate(dem_change = dem_pct_20 - dem_pct_16) %&gt;% \n  select(state_name, county_name, dem_pct_20, dem_pct_16, dem_change) %&gt;% \n  arrange(dem_change)\nhead(mn_wi)\n\n  state_name        county_name dem_pct_20 dem_pct_16 dem_change\n1  Minnesota     Stevens County      37.80      39.55      -1.75\n2  Wisconsin      Forest County      34.06      35.12      -1.06\n3  Wisconsin    Kewaunee County      32.87      33.73      -0.86\n4  Wisconsin       Clark County      30.37      31.19      -0.82\n5  Wisconsin       Adams County      36.63      37.40      -0.77\n6  Wisconsin Trempealeau County      40.86      41.57      -0.71\n\n# c) Plot change in Dem % by county, colored by state\nggplot(mn_wi, aes(x = dem_change, y = county_name, color = state_name)) +\n  geom_point() +\n  labs(x = \"Change in Dem % (2020 vs 2016)\", y = \"County\", color = \"State\")\n\n\n\n\n\n\n# Exercise 7: summarize Demo\n# Median GOP % in 2020\nelections_small %&gt;% summarize(median(repub_pct_20))\n\n  median(repub_pct_20)\n1                68.29\n\n# Named summary\nelections_small %&gt;% summarize(median_repub = median(repub_pct_20))\n\n  median_repub\n1        68.29\n\n# Multiple summaries\nelections_small %&gt;% summarize(median_repub = median(repub_pct_20), total_votes = sum(total_votes_20))\n\n  median_repub total_votes\n1        68.29   157949293\n\n# Exercise 8: group_by + summarize\n# Summary by state\nelections_small %&gt;% \n  group_by(state_name) %&gt;% \n  summarize(median_repub = median(repub_pct_20), total_votes = sum(total_votes_20))\n\n# A tibble: 50 × 3\n   state_name           median_repub total_votes\n   &lt;chr&gt;                       &lt;dbl&gt;       &lt;int&gt;\n 1 Alabama                      70.6     2323304\n 2 Arizona                      57.9     3387326\n 3 Arkansas                     72.1     1219069\n 4 California                   44.8    17495906\n 5 Colorado                     56.2     3256953\n 6 Connecticut                  41.0     1824280\n 7 Delaware                     47.1      504010\n 8 District of Columbia          5.4      344356\n 9 Florida                      64.6    11067456\n10 Georgia                      68       4997716\n# ℹ 40 more rows\n\n# Exercise 9: DIY Summaries\n# Total votes by state, sorted\nelections_small %&gt;% \n  group_by(state_name) %&gt;% \n  summarize(total_votes_state = sum(total_votes_20)) %&gt;% \n  arrange(desc(total_votes_state))\n\n# A tibble: 50 × 2\n   state_name     total_votes_state\n   &lt;chr&gt;                      &lt;int&gt;\n 1 California              17495906\n 2 Texas                   11317911\n 3 Florida                 11067456\n 4 New York                 8616205\n 5 Pennsylvania             6925255\n 6 Illinois                 6038850\n 7 Ohio                     5922202\n 8 Michigan                 5539302\n 9 North Carolina           5524801\n10 Georgia                  4997716\n# ℹ 40 more rows\n\n# Total Dem & GOP votes by state\nelections_small %&gt;% \n  mutate(dem_votes = round(total_votes_20 * dem_pct_20 / 100),\n         rep_votes = round(total_votes_20 * repub_pct_20 / 100)) %&gt;% \n  group_by(state_name) %&gt;% \n  summarize(dem_votes = sum(dem_votes), rep_votes = sum(rep_votes))\n\n# A tibble: 50 × 3\n   state_name           dem_votes rep_votes\n   &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Alabama                 849664   1441155\n 2 Arizona                1672127   1661671\n 3 Arkansas                423919    760641\n 4 California            11109642   6006031\n 5 Colorado               1804393   1364627\n 6 Connecticut            1080677    715315\n 7 Delaware                296274    200601\n 8 District of Columbia    317324     18595\n 9 Florida                5297131   5668600\n10 Georgia                2473661   2461869\n# ℹ 40 more rows\n\n# States the Democrats won in 2020\nelections_small %&gt;% \n  mutate(dem_votes = round(total_votes_20 * dem_pct_20 / 100),\n         rep_votes = round(total_votes_20 * repub_pct_20 / 100)) %&gt;% \n  group_by(state_name) %&gt;% \n  summarize(dem_votes = sum(dem_votes), rep_votes = sum(rep_votes)) %&gt;% \n  filter(dem_votes &gt; rep_votes)\n\n# A tibble: 26 × 3\n   state_name           dem_votes rep_votes\n   &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Arizona                1672127   1661671\n 2 California            11109642   6006031\n 3 Colorado               1804393   1364627\n 4 Connecticut            1080677    715315\n 5 Delaware                296274    200601\n 6 District of Columbia    317324     18595\n 7 Georgia                2473661   2461869\n 8 Hawaii                  366121    196865\n 9 Illinois               3471916   2446931\n10 Maine                   430466    359897\n# ℹ 16 more rows\n\n# Exercise 10: Practice on World Cup data\nworld_cup &lt;- read.csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-11-29/worldcups.csv\")\n\n# Years Brazil won\nworld_cup %&gt;% filter(winner == \"Brazil\") %&gt;% pull(year)\n\n[1] 1958 1962 1970 1994 2002\n\n# Top 6 World Cups by attendance\nworld_cup %&gt;% arrange(desc(attendance)) %&gt;% head(6)\n\n  year               host  winner    second       third      fourth\n1 1994                USA  Brazil     Italy      Sweden    Bulgaria\n2 2014             Brazil Germany Argentina Netherlands      Brazil\n3 2006            Germany   Italy    France     Germany    Portugal\n4 2018             Russia  France   Croatia     Belgium     England\n5 1998             France  France    Brazil     Croatia Netherlands\n6 2002 Japan, South Korea  Brazil   Germany      Turkey South Korea\n  goals_scored teams games attendance\n1          141    24    52    3568567\n2          171    32    64    3441450\n3          147    32    64    3367000\n4          169    32    64    3031768\n5          171    32    64    2859234\n6          161    32    64    2724604\n\n# Univariate plot of goals_scored\nggplot(world_cup, aes(x = goals_scored)) +\n  geom_histogram(binwidth = 10) +\n  labs(x = \"Goals Scored\", y = \"Count of World Cups\")\n\n\n\n\n\n\n# Summary stats: min, median, max goals_scored\nworld_cup %&gt;% summarize(\n  min_goals = min(goals_scored),\n  median_goals = median(goals_scored),\n  max_goals = max(goals_scored)\n)\n\n  min_goals median_goals max_goals\n1        70          126       171\n\n# Bivariate: goals_scored over the years\nggplot(world_cup, aes(x = year, y = goals_scored)) +\n  geom_point() +\n  geom_line() +\n  labs(x = \"Year\", y = \"Goals Scored\")",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Wrangling</span>"
    ]
  },
  {
    "objectID": "ica/ica-dates.html",
    "href": "ica/ica-dates.html",
    "title": "\n12  Dates\n",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(palmerpenguins)\nlibrary(lubridate)\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\n\n\n# Exercise 1a: More Filtering\n# Keep only Adelie and Chinstrap using %in%\npenguins %&gt;% \n  filter(species %in% c(\"Adelie\", \"Chinstrap\")) %&gt;% \n  count(species)\n\n# A tibble: 2 × 2\n  species       n\n  &lt;fct&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n\n# %in% keeps rows where species is one of the two\n\n# Keep only Adelie and Chinstrap using !=\npenguins %&gt;% \n  filter(species != \"Gentoo\") %&gt;% \n  count(species)\n\n# A tibble: 2 × 2\n  species       n\n  &lt;fct&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n\n# species != \"Gentoo\" drops Gentoo, leaving the other two\n\n# Exercise 1b: Handling NAs\n# Count how many penguins missing body_mass_g\npenguins %&gt;% summarize(missing = sum(is.na(body_mass_g)))\n\n# A tibble: 1 × 1\n  missing\n    &lt;int&gt;\n1       2\n\n# Remove only those missing body_mass_g\npenguins_w_body_mass &lt;- penguins %&gt;% filter(!is.na(body_mass_g))\n# Remove rows missing *any* variable (use sparingly!)\npenguins_complete &lt;- penguins %&gt;% na.omit()\n# na.omit() drops rows with ANY NA; use only when you truly need complete cases\n\n# Exercise 2: More Selecting\n# Keep everything except year and island\npenguins %&gt;% select(-year, -island)\n\n# A tibble: 344 × 6\n   species bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1 Adelie            39.1          18.7               181        3750 male  \n 2 Adelie            39.5          17.4               186        3800 female\n 3 Adelie            40.3          18                 195        3250 female\n 4 Adelie            NA            NA                  NA          NA &lt;NA&gt;  \n 5 Adelie            36.7          19.3               193        3450 female\n 6 Adelie            39.3          20.6               190        3650 male  \n 7 Adelie            38.9          17.8               181        3625 female\n 8 Adelie            39.2          19.6               195        4675 male  \n 9 Adelie            34.1          18.1               193        3475 &lt;NA&gt;  \n10 Adelie            42            20.2               190        4250 &lt;NA&gt;  \n# ℹ 334 more rows\n\n# Keep species and all measurements in millimeters\npenguins %&gt;% select(species, ends_with(\"_mm\"))\n\n# A tibble: 344 × 4\n   species bill_length_mm bill_depth_mm flipper_length_mm\n   &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;\n 1 Adelie            39.1          18.7               181\n 2 Adelie            39.5          17.4               186\n 3 Adelie            40.3          18                 195\n 4 Adelie            NA            NA                  NA\n 5 Adelie            36.7          19.3               193\n 6 Adelie            39.3          20.6               190\n 7 Adelie            38.9          17.8               181\n 8 Adelie            39.2          19.6               195\n 9 Adelie            34.1          18.1               193\n10 Adelie            42            20.2               190\n# ℹ 334 more rows\n\n# Keep species and all bill-related measurements\npenguins %&gt;% select(species, starts_with(\"bill\"))\n\n# A tibble: 344 × 3\n   species bill_length_mm bill_depth_mm\n   &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;\n 1 Adelie            39.1          18.7\n 2 Adelie            39.5          17.4\n 3 Adelie            40.3          18  \n 4 Adelie            NA            NA  \n 5 Adelie            36.7          19.3\n 6 Adelie            39.3          20.6\n 7 Adelie            38.9          17.8\n 8 Adelie            39.2          19.6\n 9 Adelie            34.1          18.1\n10 Adelie            42            20.2\n# ℹ 334 more rows\n\n# Keep species and all length-related measurements\npenguins %&gt;% select(species, contains(\"length\"))\n\n# A tibble: 344 × 3\n   species bill_length_mm flipper_length_mm\n   &lt;fct&gt;            &lt;dbl&gt;             &lt;int&gt;\n 1 Adelie            39.1               181\n 2 Adelie            39.5               186\n 3 Adelie            40.3               195\n 4 Adelie            NA                  NA\n 5 Adelie            36.7               193\n 6 Adelie            39.3               190\n 7 Adelie            38.9               181\n 8 Adelie            39.2               195\n 9 Adelie            34.1               193\n10 Adelie            42                 190\n# ℹ 334 more rows\n\n# Exercise 3: Arranging, Counting, Grouping by Multiple\n# Sort by species then island\npenguins %&gt;% arrange(species, island)\n\n# A tibble: 344 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Biscoe           37.8          18.3               174        3400\n 2 Adelie  Biscoe           37.7          18.7               180        3600\n 3 Adelie  Biscoe           35.9          19.2               189        3800\n 4 Adelie  Biscoe           38.2          18.1               185        3950\n 5 Adelie  Biscoe           38.8          17.2               180        3800\n 6 Adelie  Biscoe           35.3          18.9               187        3800\n 7 Adelie  Biscoe           40.6          18.6               183        3550\n 8 Adelie  Biscoe           40.5          17.9               187        3200\n 9 Adelie  Biscoe           37.9          18.6               172        3150\n10 Adelie  Biscoe           40.5          18.9               180        3950\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n# Count male/female by species\npenguins %&gt;% count(species, sex)\n\n# A tibble: 8 × 3\n  species   sex        n\n  &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;\n1 Adelie    female    73\n2 Adelie    male      73\n3 Adelie    &lt;NA&gt;       6\n4 Chinstrap female    34\n5 Chinstrap male      34\n6 Gentoo    female    58\n7 Gentoo    male      61\n8 Gentoo    &lt;NA&gt;       5\n\n# Average body mass by species and sex\npenguins %&gt;% \n  group_by(species, sex) %&gt;% \n  summarize(mean_mass = mean(body_mass_g, na.rm = TRUE))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   species [3]\n  species   sex    mean_mass\n  &lt;fct&gt;     &lt;fct&gt;      &lt;dbl&gt;\n1 Adelie    female     3369.\n2 Adelie    male       4043.\n3 Adelie    &lt;NA&gt;       3540 \n4 Chinstrap female     3527.\n5 Chinstrap male       3939.\n6 Gentoo    female     4680.\n7 Gentoo    male       5485.\n8 Gentoo    &lt;NA&gt;       4588.\n\n# Exercise 4: Dates with lubridate\ntoday &lt;- as.Date(today())\nclass(today)           # \"Date\"\n\n[1] \"Date\"\n\nyear(today)            # 2025\n\n[1] 2025\n\nmonth(today)           # 5\n\n[1] 5\n\nmonth(today, label=TRUE)  # \"May\"\n\n[1] May\n12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec\n\nweek(today)            # 19 (ISO week)\n\n[1] 19\n\nmday(today)            # 7 (day of month)\n\n[1] 13\n\nyday(today)            # 127 (day of year)\n\n[1] 133\n\nwday(today)            # 4 (1 = Sunday)\n\n[1] 3\n\nwday(today, label=TRUE)  # \"Wed\"\n\n[1] Tue\nLevels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat\n\ntoday &gt;= ymd(\"2024-02-14\")  # TRUE\n\n[1] TRUE\n\ntoday &lt;  ymd(\"2024-02-14\")  # FALSE\n\n[1] FALSE\n\n# Exercise 5: Birthdays dataset\ndata(\"Birthdays\")\n# How many days of data per state?\nBirthdays %&gt;% count(state)\n\n   state    n\n1     AK 7306\n2     AL 7312\n3     AR 7310\n4     AZ 7310\n5     CA 7325\n6     CO 7305\n7     CT 7312\n8     DC 7311\n9     DE 7307\n10    FL 7307\n11    GA 7314\n12    HI 7306\n13    IA 7306\n14    ID 7306\n15    IL 7314\n16    IN 7311\n17    KS 7311\n18    KY 7313\n19    LA 7309\n20    MA 7315\n21    MD 7311\n22    ME 7309\n23    MI 7323\n24    MN 7315\n25    MO 7309\n26    MS 7310\n27    MT 7305\n28    NC 7307\n29    ND 7305\n30    NE 7305\n31    NH 7308\n32    NJ 7321\n33    NM 7308\n34    NV 7307\n35    NY 7333\n36    OH 7319\n37    OK 7306\n38    OR 7307\n39    PA 7330\n40    RI 7305\n41    SC 7314\n42    SD 7305\n43    TN 7308\n44    TX 7330\n45    UT 7307\n46    VA 7310\n47    VT 7305\n48    WA 7306\n49    WI 7311\n50    WV 7310\n51    WY 7305\n\n# Total births overall\nBirthdays %&gt;% summarize(total_births = sum(births))\n\n  total_births\n1     70486538\n\n# Total births per state, sorted low→high\nBirthdays %&gt;% \n  group_by(state) %&gt;% \n  summarize(total = sum(births)) %&gt;% \n  arrange(total)\n\n# A tibble: 51 × 2\n   state  total\n   &lt;chr&gt;  &lt;int&gt;\n 1 VT    147886\n 2 WY    154019\n 3 AK    185385\n 4 DE    188705\n 5 SD    235734\n 6 ND    238696\n 7 NV    241470\n 8 MT    253884\n 9 NH    264984\n10 RI    265038\n# ℹ 41 more rows\n\n# Exercise 6: daily_births and plot\ndaily_births &lt;- Birthdays %&gt;% \n  group_by(date, wday) %&gt;% \n  summarize(total_births = sum(births), .groups = \"drop\")\n\nggplot(daily_births, aes(x = date, y = total_births, color = wday)) +\n  geom_line() +\n  labs(x = \"Date\", y = \"Total Births\", color = \"Weekday\")\n\n\n\n\n\n\n# Exercise 7a: Monthly births over time\nmonthly_births &lt;- Birthdays %&gt;% \n  mutate(month = month(date, label = TRUE)) %&gt;% \n  group_by(year, month) %&gt;% \n  summarize(births = sum(births), .groups = \"drop\")\n\nggplot(monthly_births, aes(x = month, y = births, group = year, color = factor(year))) +\n  geom_line() +\n  labs(x = \"Month\", y = \"Total Births\", color = \"Year\")\n\n\n\n\n\n\n# Seasonal peaks and troughs emerge, consistent patterns year-to-year.\n\n# Exercise 7b: Weekly births in 1988 by state (drop week 53)\nweekly_1988 &lt;- Birthdays %&gt;% \n  filter(year == 1988) %&gt;% \n  mutate(week = week(date)) %&gt;% \n  filter(week != 53) %&gt;% \n  group_by(state, week) %&gt;% \n  summarize(births = sum(births), .groups = \"drop\")\n\nggplot(weekly_1988, aes(x = week, y = births, color = state)) +\n  geom_line() +\n  labs(x = \"Week\", y = \"Total Births\", color = \"State\")\n\n\n\n\n\n\n# Notice seasonal trends differ by state\n\n# Exercise 7c: Compare MN vs LA in 1988\nmn_la_1988 &lt;- weekly_1988 %&gt;% filter(state %in% c(\"MN\", \"LA\"))\n\nggplot(mn_la_1988, aes(x = week, y = births, color = state)) +\n  geom_line() +\n  labs(x = \"Week\", y = \"Total Births\", color = \"State\")\n\n\n\n\n\n\n# MN and LA show different seasonal patterns reflecting climate differences\n\n# Exercise 8: More Practice\n# a) MA in 1979, sorted by births descending\nBirthdays %&gt;% \n  filter(state == \"MA\", year == 1979) %&gt;% \n  arrange(desc(births))\n\n    state year month day       date  wday births\n1      MA 1979     9  28 1979-09-28   Fri    262\n2      MA 1979     9  11 1979-09-11  Tues    252\n3      MA 1979    12  28 1979-12-28   Fri    249\n4      MA 1979     9  26 1979-09-26   Wed    246\n5      MA 1979     7  24 1979-07-24  Tues    245\n6      MA 1979     4  27 1979-04-27   Fri    243\n7      MA 1979     8   6 1979-08-06   Mon    243\n8      MA 1979    10   2 1979-10-02  Tues    239\n9      MA 1979     6  29 1979-06-29   Fri    238\n10     MA 1979     8  23 1979-08-23 Thurs    238\n11     MA 1979     2  23 1979-02-23   Fri    237\n12     MA 1979     5   1 1979-05-01  Tues    236\n13     MA 1979     7  18 1979-07-18   Wed    236\n14     MA 1979     8   2 1979-08-02 Thurs    236\n15     MA 1979     5  24 1979-05-24 Thurs    235\n16     MA 1979     8   1 1979-08-01   Wed    235\n17     MA 1979     4   6 1979-04-06   Fri    234\n18     MA 1979     8  31 1979-08-31   Fri    234\n19     MA 1979     5  10 1979-05-10 Thurs    233\n20     MA 1979     8  15 1979-08-15   Wed    233\n21     MA 1979     3  23 1979-03-23   Fri    232\n22     MA 1979     6  12 1979-06-12  Tues    232\n23     MA 1979     7  10 1979-07-10  Tues    232\n24     MA 1979     7  25 1979-07-25   Wed    232\n25     MA 1979     6  27 1979-06-27   Wed    231\n26     MA 1979     7  27 1979-07-27   Fri    231\n27     MA 1979    10   5 1979-10-05   Fri    231\n28     MA 1979     5  23 1979-05-23   Wed    230\n29     MA 1979     9  19 1979-09-19   Wed    229\n30     MA 1979     2  16 1979-02-16   Fri    228\n31     MA 1979     8  16 1979-08-16 Thurs    228\n32     MA 1979     8  24 1979-08-24   Fri    228\n33     MA 1979     9  17 1979-09-17   Mon    228\n34     MA 1979     4  30 1979-04-30   Mon    227\n35     MA 1979     6  26 1979-06-26  Tues    227\n36     MA 1979     7  20 1979-07-20   Fri    227\n37     MA 1979     8  10 1979-08-10   Fri    227\n38     MA 1979     8  14 1979-08-14  Tues    227\n39     MA 1979     9  24 1979-09-24   Mon    227\n40     MA 1979     3   5 1979-03-05   Mon    226\n41     MA 1979     6  21 1979-06-21 Thurs    226\n42     MA 1979     2   2 1979-02-02   Fri    225\n43     MA 1979     2  27 1979-02-27  Tues    225\n44     MA 1979     4   9 1979-04-09   Mon    225\n45     MA 1979     5   8 1979-05-08  Tues    225\n46     MA 1979     7  17 1979-07-17  Tues    225\n47     MA 1979     9  14 1979-09-14   Fri    225\n48     MA 1979     9  20 1979-09-20 Thurs    225\n49     MA 1979     6  11 1979-06-11   Mon    224\n50     MA 1979    11   9 1979-11-09   Fri    224\n51     MA 1979     2  14 1979-02-14   Wed    223\n52     MA 1979     7  23 1979-07-23   Mon    223\n53     MA 1979     8   9 1979-08-09 Thurs    223\n54     MA 1979     1   3 1979-01-03   Wed    222\n55     MA 1979     1  22 1979-01-22   Mon    222\n56     MA 1979     6  19 1979-06-19  Tues    222\n57     MA 1979     9   4 1979-09-04  Tues    222\n58     MA 1979     9  21 1979-09-21   Fri    222\n59     MA 1979    10  23 1979-10-23  Tues    222\n60     MA 1979     2   6 1979-02-06  Tues    221\n61     MA 1979     3  19 1979-03-19   Mon    221\n62     MA 1979     4  25 1979-04-25   Wed    221\n63     MA 1979     8  20 1979-08-20   Mon    221\n64     MA 1979     9  18 1979-09-18  Tues    221\n65     MA 1979    11   6 1979-11-06  Tues    221\n66     MA 1979     2  20 1979-02-20  Tues    220\n67     MA 1979     4  26 1979-04-26 Thurs    220\n68     MA 1979     8  13 1979-08-13   Mon    220\n69     MA 1979     8  17 1979-08-17   Fri    220\n70     MA 1979     9  10 1979-09-10   Mon    220\n71     MA 1979    10  25 1979-10-25 Thurs    220\n72     MA 1979    10  26 1979-10-26   Fri    220\n73     MA 1979     3   6 1979-03-06  Tues    219\n74     MA 1979     6   1 1979-06-01   Fri    219\n75     MA 1979     8  29 1979-08-29   Wed    219\n76     MA 1979     9  27 1979-09-27 Thurs    219\n77     MA 1979    10   1 1979-10-01   Mon    219\n78     MA 1979    11  19 1979-11-19   Mon    219\n79     MA 1979     4  24 1979-04-24  Tues    218\n80     MA 1979     5  21 1979-05-21   Mon    218\n81     MA 1979     6  28 1979-06-28 Thurs    218\n82     MA 1979     7   9 1979-07-09   Mon    218\n83     MA 1979     8   3 1979-08-03   Fri    218\n84     MA 1979     9   6 1979-09-06 Thurs    218\n85     MA 1979    10   3 1979-10-03   Wed    218\n86     MA 1979     6   7 1979-06-07 Thurs    217\n87     MA 1979     6  14 1979-06-14 Thurs    217\n88     MA 1979     7   3 1979-07-03  Tues    217\n89     MA 1979     8   8 1979-08-08   Wed    217\n90     MA 1979    10   4 1979-10-04 Thurs    217\n91     MA 1979    10  22 1979-10-22   Mon    217\n92     MA 1979    10  29 1979-10-29   Mon    217\n93     MA 1979    10   9 1979-10-09  Tues    216\n94     MA 1979     1  16 1979-01-16  Tues    215\n95     MA 1979     3  13 1979-03-13  Tues    215\n96     MA 1979     5   4 1979-05-04   Fri    215\n97     MA 1979     5  14 1979-05-14   Mon    215\n98     MA 1979     6  22 1979-06-22   Fri    215\n99     MA 1979     3  30 1979-03-30   Fri    214\n100    MA 1979     6   5 1979-06-05  Tues    214\n101    MA 1979    12  14 1979-12-14   Fri    214\n102    MA 1979    12  27 1979-12-27 Thurs    214\n103    MA 1979     3   8 1979-03-08 Thurs    213\n104    MA 1979     4  18 1979-04-18   Wed    213\n105    MA 1979     8  28 1979-08-28  Tues    213\n106    MA 1979    10  12 1979-10-12   Fri    213\n107    MA 1979    11  14 1979-11-14   Wed    213\n108    MA 1979    12  31 1979-12-31   Mon    213\n109    MA 1979     4  23 1979-04-23   Mon    212\n110    MA 1979     5  11 1979-05-11   Fri    212\n111    MA 1979     5  18 1979-05-18   Fri    212\n112    MA 1979     5  30 1979-05-30   Wed    212\n113    MA 1979     7   6 1979-07-06   Fri    212\n114    MA 1979     7  19 1979-07-19 Thurs    212\n115    MA 1979    10  17 1979-10-17   Wed    212\n116    MA 1979    10  24 1979-10-24   Wed    212\n117    MA 1979    12  11 1979-12-11  Tues    212\n118    MA 1979     2   5 1979-02-05   Mon    211\n119    MA 1979     3   7 1979-03-07   Wed    211\n120    MA 1979     3  29 1979-03-29 Thurs    211\n121    MA 1979     9   7 1979-09-07   Fri    211\n122    MA 1979    10  30 1979-10-30  Tues    211\n123    MA 1979     7   2 1979-07-02   Mon    210\n124    MA 1979     1  15 1979-01-15   Mon    209\n125    MA 1979     3  26 1979-03-26   Mon    209\n126    MA 1979     4   4 1979-04-04   Wed    209\n127    MA 1979     7  15 1979-07-15   Sun    209\n128    MA 1979     2   8 1979-02-08 Thurs    208\n129    MA 1979     4  11 1979-04-11   Wed    208\n130    MA 1979     6   6 1979-06-06   Wed    208\n131    MA 1979     6  30 1979-06-30   Sat    208\n132    MA 1979     7  30 1979-07-30   Mon    208\n133    MA 1979     9  29 1979-09-29   Sat    208\n134    MA 1979    10  10 1979-10-10   Wed    208\n135    MA 1979     2   9 1979-02-09   Fri    207\n136    MA 1979     3   9 1979-03-09   Fri    207\n137    MA 1979     4  20 1979-04-20   Fri    207\n138    MA 1979     6  20 1979-06-20   Wed    207\n139    MA 1979     7  26 1979-07-26 Thurs    207\n140    MA 1979     8  21 1979-08-21  Tues    207\n141    MA 1979     9  25 1979-09-25  Tues    207\n142    MA 1979    11  26 1979-11-26   Mon    207\n143    MA 1979     1   2 1979-01-02  Tues    206\n144    MA 1979     2  22 1979-02-22 Thurs    206\n145    MA 1979     7  12 1979-07-12 Thurs    206\n146    MA 1979     7  13 1979-07-13   Fri    206\n147    MA 1979     9   5 1979-09-05   Wed    206\n148    MA 1979     5  25 1979-05-25   Fri    205\n149    MA 1979    11  23 1979-11-23   Fri    205\n150    MA 1979    11  28 1979-11-28   Wed    205\n151    MA 1979    12   4 1979-12-04  Tues    205\n152    MA 1979     1  26 1979-01-26   Fri    204\n153    MA 1979     3  22 1979-03-22 Thurs    204\n154    MA 1979     5  16 1979-05-16   Wed    204\n155    MA 1979    10  16 1979-10-16  Tues    204\n156    MA 1979    11  16 1979-11-16   Fri    204\n157    MA 1979    12  24 1979-12-24   Mon    204\n158    MA 1979     3   2 1979-03-02   Fri    203\n159    MA 1979     4   3 1979-04-03  Tues    203\n160    MA 1979     4  12 1979-04-12 Thurs    203\n161    MA 1979     5   3 1979-05-03 Thurs    203\n162    MA 1979    10   6 1979-10-06   Sat    203\n163    MA 1979    11   1 1979-11-01 Thurs    203\n164    MA 1979    11  30 1979-11-30   Fri    203\n165    MA 1979    12  10 1979-12-10   Mon    203\n166    MA 1979    12  17 1979-12-17   Mon    203\n167    MA 1979     6   8 1979-06-08   Fri    202\n168    MA 1979     6  15 1979-06-15   Fri    202\n169    MA 1979    11  11 1979-11-11   Sun    202\n170    MA 1979     1  29 1979-01-29   Mon    201\n171    MA 1979     3  21 1979-03-21   Wed    201\n172    MA 1979     3  28 1979-03-28   Wed    201\n173    MA 1979     4  10 1979-04-10  Tues    201\n174    MA 1979     5   2 1979-05-02   Wed    201\n175    MA 1979    10  15 1979-10-15   Mon    201\n176    MA 1979    11   7 1979-11-07   Wed    201\n177    MA 1979     1  18 1979-01-18 Thurs    200\n178    MA 1979     4  19 1979-04-19 Thurs    200\n179    MA 1979     7  11 1979-07-11   Wed    200\n180    MA 1979     7  21 1979-07-21   Sat    200\n181    MA 1979     8   5 1979-08-05   Sun    200\n182    MA 1979    10  19 1979-10-19   Fri    200\n183    MA 1979    11   2 1979-11-02   Fri    200\n184    MA 1979    11  15 1979-11-15 Thurs    200\n185    MA 1979    11  20 1979-11-20  Tues    200\n186    MA 1979     1  11 1979-01-11 Thurs    199\n187    MA 1979     2  12 1979-02-12   Mon    199\n188    MA 1979     3  14 1979-03-14   Wed    199\n189    MA 1979     5  22 1979-05-22  Tues    199\n190    MA 1979     2  26 1979-02-26   Mon    198\n191    MA 1979     3  15 1979-03-15 Thurs    198\n192    MA 1979     6  13 1979-06-13   Wed    198\n193    MA 1979     6  18 1979-06-18   Mon    198\n194    MA 1979     7  31 1979-07-31  Tues    198\n195    MA 1979     8  26 1979-08-26   Sun    198\n196    MA 1979     9  13 1979-09-13 Thurs    198\n197    MA 1979    11  13 1979-11-13  Tues    198\n198    MA 1979    12   7 1979-12-07   Fri    198\n199    MA 1979     5  31 1979-05-31 Thurs    197\n200    MA 1979    11  27 1979-11-27  Tues    197\n201    MA 1979    12   6 1979-12-06 Thurs    197\n202    MA 1979     2  21 1979-02-21   Wed    196\n203    MA 1979     4  17 1979-04-17  Tues    196\n204    MA 1979     4  21 1979-04-21   Sat    196\n205    MA 1979     6  25 1979-06-25   Mon    196\n206    MA 1979     8   4 1979-08-04   Sat    196\n207    MA 1979     8   7 1979-08-07  Tues    196\n208    MA 1979    10   8 1979-10-08   Mon    196\n209    MA 1979    10  11 1979-10-11 Thurs    196\n210    MA 1979     1  13 1979-01-13   Sat    195\n211    MA 1979     4  13 1979-04-13   Fri    195\n212    MA 1979     1  19 1979-01-19   Fri    194\n213    MA 1979     3  16 1979-03-16   Fri    194\n214    MA 1979     5   7 1979-05-07   Mon    194\n215    MA 1979     9  12 1979-09-12   Wed    194\n216    MA 1979    11   5 1979-11-05   Mon    194\n217    MA 1979     1  10 1979-01-10   Wed    193\n218    MA 1979     1  30 1979-01-30  Tues    193\n219    MA 1979     3  11 1979-03-11   Sun    193\n220    MA 1979     6  17 1979-06-17   Sun    193\n221    MA 1979     8  27 1979-08-27   Mon    193\n222    MA 1979     9   9 1979-09-09   Sun    193\n223    MA 1979    12  13 1979-12-13 Thurs    193\n224    MA 1979     1   5 1979-01-05   Fri    192\n225    MA 1979     2  19 1979-02-19   Mon    192\n226    MA 1979     3   1 1979-03-01 Thurs    192\n227    MA 1979     7  14 1979-07-14   Sat    192\n228    MA 1979    11  29 1979-11-29 Thurs    192\n229    MA 1979     1   8 1979-01-08   Mon    191\n230    MA 1979     1  12 1979-01-12   Fri    191\n231    MA 1979     2  15 1979-02-15 Thurs    191\n232    MA 1979     3  24 1979-03-24   Sat    191\n233    MA 1979    10  28 1979-10-28   Sun    191\n234    MA 1979     4   2 1979-04-02   Mon    190\n235    MA 1979     5  27 1979-05-27   Sun    190\n236    MA 1979    12   3 1979-12-03   Mon    190\n237    MA 1979    12  12 1979-12-12   Wed    190\n238    MA 1979     4   1 1979-04-01   Sun    189\n239    MA 1979     4   5 1979-04-05 Thurs    189\n240    MA 1979     8  22 1979-08-22   Wed    189\n241    MA 1979    12  26 1979-12-26   Wed    189\n242    MA 1979     1  24 1979-01-24   Wed    188\n243    MA 1979     7  16 1979-07-16   Mon    188\n244    MA 1979    11  21 1979-11-21   Wed    188\n245    MA 1979     4  16 1979-04-16   Mon    187\n246    MA 1979     6  10 1979-06-10   Sun    187\n247    MA 1979     8  11 1979-08-11   Sat    187\n248    MA 1979     2   7 1979-02-07   Wed    186\n249    MA 1979     3  17 1979-03-17   Sat    186\n250    MA 1979     3  27 1979-03-27  Tues    186\n251    MA 1979     4   7 1979-04-07   Sat    186\n252    MA 1979     5   9 1979-05-09   Wed    186\n253    MA 1979     5  15 1979-05-15  Tues    186\n254    MA 1979     5  17 1979-05-17 Thurs    186\n255    MA 1979     7   7 1979-07-07   Sat    186\n256    MA 1979     9  15 1979-09-15   Sat    186\n257    MA 1979    10   7 1979-10-07   Sun    186\n258    MA 1979     3  12 1979-03-12   Mon    185\n259    MA 1979     3  20 1979-03-20  Tues    185\n260    MA 1979    12   5 1979-12-05   Wed    185\n261    MA 1979     1   4 1979-01-04 Thurs    184\n262    MA 1979     2   1 1979-02-01 Thurs    184\n263    MA 1979     5  29 1979-05-29  Tues    184\n264    MA 1979     9   3 1979-09-03   Mon    184\n265    MA 1979    11   8 1979-11-08 Thurs    184\n266    MA 1979    12  19 1979-12-19   Wed    184\n267    MA 1979    12  21 1979-12-21   Fri    184\n268    MA 1979     1  23 1979-01-23  Tues    183\n269    MA 1979     4  28 1979-04-28   Sat    183\n270    MA 1979     6  24 1979-06-24   Sun    183\n271    MA 1979     7  28 1979-07-28   Sat    183\n272    MA 1979     7  29 1979-07-29   Sun    183\n273    MA 1979     8  19 1979-08-19   Sun    183\n274    MA 1979    10  21 1979-10-21   Sun    183\n275    MA 1979    10  18 1979-10-18 Thurs    182\n276    MA 1979     1   9 1979-01-09  Tues    180\n277    MA 1979     2  11 1979-02-11   Sun    180\n278    MA 1979     5  19 1979-05-19   Sat    180\n279    MA 1979     5  28 1979-05-28   Mon    180\n280    MA 1979     6   3 1979-06-03   Sun    180\n281    MA 1979    10  20 1979-10-20   Sat    180\n282    MA 1979    10  31 1979-10-31   Wed    180\n283    MA 1979    12  18 1979-12-18  Tues    180\n284    MA 1979     1  31 1979-01-31   Wed    179\n285    MA 1979     3  10 1979-03-10   Sat    179\n286    MA 1979     4  29 1979-04-29   Sun    179\n287    MA 1979     7   5 1979-07-05 Thurs    179\n288    MA 1979     8  30 1979-08-30 Thurs    179\n289    MA 1979     1  25 1979-01-25 Thurs    178\n290    MA 1979     5  26 1979-05-26   Sat    178\n291    MA 1979    10  14 1979-10-14   Sun    178\n292    MA 1979     2  28 1979-02-28   Wed    177\n293    MA 1979     7   8 1979-07-08   Sun    177\n294    MA 1979     2  25 1979-02-25   Sun    176\n295    MA 1979     4  15 1979-04-15   Sun    176\n296    MA 1979     9  22 1979-09-22   Sat    176\n297    MA 1979     6  23 1979-06-23   Sat    175\n298    MA 1979     7  22 1979-07-22   Sun    175\n299    MA 1979    10  27 1979-10-27   Sat    175\n300    MA 1979     2  10 1979-02-10   Sat    174\n301    MA 1979     8  25 1979-08-25   Sat    174\n302    MA 1979     9  16 1979-09-16   Sun    174\n303    MA 1979    11  12 1979-11-12   Mon    174\n304    MA 1979    11  17 1979-11-17   Sat    174\n305    MA 1979     6   2 1979-06-02   Sat    173\n306    MA 1979     7   1 1979-07-01   Sun    173\n307    MA 1979    10  13 1979-10-13   Sat    173\n308    MA 1979     3  25 1979-03-25   Sun    172\n309    MA 1979     6  16 1979-06-16   Sat    172\n310    MA 1979    11  10 1979-11-10   Sat    172\n311    MA 1979     2  13 1979-02-13  Tues    171\n312    MA 1979     3  31 1979-03-31   Sat    171\n313    MA 1979     6   9 1979-06-09   Sat    171\n314    MA 1979    11  25 1979-11-25   Sun    171\n315    MA 1979    12  22 1979-12-22   Sat    171\n316    MA 1979     5   6 1979-05-06   Sun    170\n317    MA 1979     9   2 1979-09-02   Sun    170\n318    MA 1979     1   6 1979-01-06   Sat    169\n319    MA 1979     8  12 1979-08-12   Sun    169\n320    MA 1979     9   8 1979-09-08   Sat    169\n321    MA 1979    11   3 1979-11-03   Sat    169\n322    MA 1979    12  15 1979-12-15   Sat    169\n323    MA 1979     2  18 1979-02-18   Sun    168\n324    MA 1979     9   1 1979-09-01   Sat    168\n325    MA 1979    11  18 1979-11-18   Sun    166\n326    MA 1979    12   1 1979-12-01   Sat    166\n327    MA 1979     2  24 1979-02-24   Sat    165\n328    MA 1979     4  22 1979-04-22   Sun    165\n329    MA 1979     4  14 1979-04-14   Sat    164\n330    MA 1979     7   4 1979-07-04   Wed    164\n331    MA 1979     9  23 1979-09-23   Sun    164\n332    MA 1979    12   2 1979-12-02   Sun    164\n333    MA 1979     6   4 1979-06-04   Mon    163\n334    MA 1979    12  16 1979-12-16   Sun    163\n335    MA 1979     1  21 1979-01-21   Sun    162\n336    MA 1979     1  27 1979-01-27   Sat    162\n337    MA 1979     5  12 1979-05-12   Sat    162\n338    MA 1979     9  30 1979-09-30   Sun    162\n339    MA 1979    12  29 1979-12-29   Sat    161\n340    MA 1979    12  23 1979-12-23   Sun    160\n341    MA 1979     2   3 1979-02-03   Sat    159\n342    MA 1979     8  18 1979-08-18   Sat    159\n343    MA 1979     1  20 1979-01-20   Sat    158\n344    MA 1979     2  17 1979-02-17   Sat    156\n345    MA 1979     3   3 1979-03-03   Sat    156\n346    MA 1979     3   4 1979-03-04   Sun    156\n347    MA 1979    11  24 1979-11-24   Sat    156\n348    MA 1979    12   9 1979-12-09   Sun    156\n349    MA 1979     1   7 1979-01-07   Sun    155\n350    MA 1979     1  14 1979-01-14   Sun    155\n351    MA 1979     1  17 1979-01-17   Wed    155\n352    MA 1979     4   8 1979-04-08   Sun    155\n353    MA 1979    11   4 1979-11-04   Sun    155\n354    MA 1979    12  20 1979-12-20 Thurs    154\n355    MA 1979     2   4 1979-02-04   Sun    153\n356    MA 1979     5  20 1979-05-20   Sun    153\n357    MA 1979     3  18 1979-03-18   Sun    152\n358    MA 1979    12  30 1979-12-30   Sun    152\n359    MA 1979    12   8 1979-12-08   Sat    151\n360    MA 1979    12  25 1979-12-25  Tues    150\n361    MA 1979     5   5 1979-05-05   Sat    148\n362    MA 1979    11  22 1979-11-22 Thurs    147\n363    MA 1979     1  28 1979-01-28   Sun    146\n364    MA 1979     1   1 1979-01-01   Mon    144\n365    MA 1979     5  13 1979-05-13   Sun    143\n\n# b) Top 5 states by total births from 1979-09-09 to 1979-09-12\nBirthdays %&gt;% \n  filter(date &gt;= ymd(\"1979-09-09\"), date &lt;= ymd(\"1979-09-12\")) %&gt;% \n  group_by(state) %&gt;% \n  summarize(total = sum(births), .groups = \"drop\") %&gt;% \n  arrange(desc(total)) %&gt;% \n  head(5)\n\n# A tibble: 5 × 2\n  state total\n  &lt;chr&gt; &lt;int&gt;\n1 CA     4422\n2 TX     3151\n3 NY     2621\n4 IL     2235\n5 OH     1938",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Dates</span>"
    ]
  },
  {
    "objectID": "ica/ica-reshaping.html",
    "href": "ica/ica-reshaping.html",
    "title": "\n13  Reshaping\n",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidyr)\nlibrary(fivethirtyeight)\ninstall.packages('fivethirtyeightdata', repos =\n'https://fivethirtyeightdata.github.io/drat/', type = 'source')\n\n\n# Exercise 1: Sleep study reshaping & plotting\nsleep_wide &lt;- read.csv(\"https://mac-stat.github.io/data/sleep_wide.csv\")\n\n# Part a: units are subject × day observations once in long format\n# Part b/c: pivot to long, fix types\nsleep_long &lt;- sleep_wide %&gt;%\n  pivot_longer(\n    cols = starts_with(\"day_\"),\n    names_to = \"day\",\n    names_prefix = \"day_\",\n    values_to = \"reaction_time\"\n  ) %&gt;%\n  mutate(\n    Subject = as.factor(Subject),\n    day     = as.numeric(day)\n  )\n\nhead(sleep_long, 3)  # check first 3 rows\n\n# A tibble: 3 × 3\n  Subject   day reaction_time\n  &lt;fct&gt;   &lt;dbl&gt;         &lt;dbl&gt;\n1 308         0          250.\n2 308         1          259.\n3 308         2          251.\n\n# Plot all subjects on one panel\nggplot(sleep_long, aes(x = day, y = reaction_time, group = Subject, color = Subject)) +\n  geom_line() +\n  labs(\n    x     = \"Days of Sleep Restriction\",\n    y     = \"Reaction Time (ms)\",\n    title = \"Reaction Time by Day for Each Subject\"\n  )\n\n\n\n\n\n\n# Facet by subject for separate panels\nggplot(sleep_long, aes(x = day, y = reaction_time)) +\n  geom_line() +\n  facet_wrap(~ Subject) +\n  labs(\n    x     = \"Days\",\n    y     = \"Reaction Time (ms)\",\n    title = \"Individual Subject Trajectories\"\n  )\n\n\n\n\n\n\n# Part 4: pivot back to wide\nsleep_wide2 &lt;- sleep_long %&gt;%\n  pivot_wider(\n    names_from   = day,\n    names_prefix = \"day_\",\n    values_from  = reaction_time\n  )\nhead(sleep_wide2)\n\n# A tibble: 6 × 11\n  Subject day_0 day_1 day_2 day_3 day_4 day_5 day_6 day_7 day_8 day_9\n  &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 308      250.  259.  251.  321.  357.  415.  382.  290.  431.  466.\n2 309      223.  205.  203.  205.  208.  216.  214.  218.  224.  237.\n3 310      199.  194.  234.  233.  229.  220.  235.  256.  261.  248.\n4 330      322.  300.  284.  285.  286.  298.  280.  318.  305.  354.\n5 331      288.  285   302.  320.  316.  293.  290.  335.  294.  372.\n6 332      235.  243.  273.  310.  317.  310   454.  347.  330.  254.\n\n# Exercise 5: Billboard charts\ndata(\"billboard\")\n\n# Part a: wk2 vs wk1 scatter with 45° reference\nggplot(billboard, aes(x = wk1, y = wk2)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\") +\n  labs(\n    x     = \"Week 1 Rank\",\n    y     = \"Week 2 Rank\",\n    title = \"Billboard: Week 2 vs Week 1 Rankings\"\n  )\n\nWarning: Removed 5 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n# Part b: songs that improved (rank↓)\nimproved &lt;- billboard %&gt;% filter(wk2 &lt; wk1)\nimproved\n\n# A tibble: 255 × 79\n   artist     track date.entered   wk1   wk2   wk3   wk4   wk5   wk6   wk7   wk8\n   &lt;chr&gt;      &lt;chr&gt; &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 2 Pac      Baby… 2000-02-26      87    82    72    77    87    94    99    NA\n 2 2Ge+her    The … 2000-09-02      91    87    92    NA    NA    NA    NA    NA\n 3 3 Doors D… Kryp… 2000-04-08      81    70    68    67    66    57    54    53\n 4 504 Boyz   Wobb… 2000-04-15      57    34    25    17    17    31    36    49\n 5 98^0       Give… 2000-08-19      51    39    34    26    26    19     2     2\n 6 Aaliyah    I Do… 2000-01-29      84    62    51    41    38    35    35    38\n 7 Aaliyah    Try … 2000-03-18      59    53    38    28    21    18    16    14\n 8 Aguilera,… Come… 2000-08-05      57    47    45    29    23    18    11     9\n 9 Aguilera,… I Tu… 2000-04-15      50    39    30    28    21    19    20    17\n10 Aguilera,… What… 1999-11-27      71    51    28    18    13    13    11     1\n# ℹ 245 more rows\n# ℹ 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;,\n#   wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;,\n#   wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;,\n#   wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;,\n#   wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;,\n#   wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;, …\n\n# Part c: entries on 1999-11-06, drop track & date.entered\nnov_1999 &lt;- billboard %&gt;%\n  filter(date.entered == as.Date(\"1999-11-06\")) %&gt;%\n  select(-track, -date.entered)\ndim(nov_1999)  # should be 2 × 77\n\n[1]  2 77\n\n# Part d: ranking over time for those two songs\nnov_long &lt;- nov_1999 %&gt;%\n  pivot_longer(\n    cols        = starts_with(\"wk\"),\n    names_to    = \"week\",\n    names_prefix= \"wk\",\n    values_to   = \"rank\"\n  ) %&gt;%\n  mutate(week = as.integer(week))\n\nggplot(nov_long, aes(x = week, y = rank, color = artist)) +\n  geom_line() +\n  labs(\n    x     = \"Week on Chart\",\n    y     = \"Rank\",\n    title = \"Chart Trajectories for Nov 6, 1999 Entrants\"\n  )\n\nWarning: Removed 79 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n# Exercise 6: The Daily Show guests\ndata(\"daily_show_guests\")\ndaily &lt;- daily_show_guests\n\n# a) Top 15 guests by total appearances\ntop_guests &lt;- daily %&gt;%\n  count(raw_guest_list, sort = TRUE) %&gt;% \n  head(15)\ntop_guests\n\n# A tibble: 15 × 2\n   raw_guest_list        n\n   &lt;chr&gt;             &lt;int&gt;\n 1 Fareed Zakaria       19\n 2 Denis Leary          17\n 3 Brian Williams       16\n 4 Paul Rudd            13\n 5 Ricky Gervais        13\n 6 Tom Brokaw           12\n 7 Bill O'Reilly        10\n 8 Reza Aslan           10\n 9 Richard Lewis        10\n10 Will Ferrell         10\n11 Sarah Vowell          9\n12 Adam Sandler          8\n13 Ben Affleck           8\n14 Louis C.K.            8\n15 Maggie Gyllenhaal     8\n\n# b) Appearances per year for those top 15\nguest_yearly &lt;- daily %&gt;%\n  count(raw_guest_list, year) %&gt;%                  # count per guest × year\n  filter(raw_guest_list %in% top_guests$raw_guest_list) %&gt;% \n  pivot_wider(\n    names_from   = year,                       # one column per year\n    values_from  = n,                          # counts go here\n    values_fill  = 0                           # fill missing with 0\n  ) %&gt;%\n  mutate(total = rowSums(across(`1999`:`2015`))) %&gt;%  # total appearances\n  arrange(desc(total))\nguest_yearly\n\n# A tibble: 15 × 19\n   raw_guest_list `1999` `2000` `2002` `2006` `2007` `2008` `2011` `2003` `2009`\n   &lt;chr&gt;           &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;\n 1 Fareed Zakaria      0      0      0      2      1      2      1      1      2\n 2 Denis Leary         1      0      2      1      1      2      2      1      1\n 3 Brian Williams      0      0      0      1      1      3      1      1      2\n 4 Paul Rudd           1      0      1      0      1      1      0      1      1\n 5 Ricky Gervais       0      0      0      2      0      1      1      0      2\n 6 Tom Brokaw          0      0      1      0      0      2      1      0      0\n 7 Bill O'Reilly       0      0      1      0      0      1      1      0      0\n 8 Reza Aslan          0      0      0      2      1      0      0      0      2\n 9 Richard Lewis       1      0      2      0      0      1      1      1      0\n10 Will Ferrell        0      1      0      1      0      0      1      1      1\n11 Sarah Vowell        0      0      1      1      0      1      1      0      1\n12 Adam Sandler        1      2      1      1      1      1      1      0      0\n13 Ben Affleck         0      0      0      1      1      0      0      2      1\n14 Louis C.K.          0      0      0      1      1      0      1      0      0\n15 Maggie Gyllen…      0      0      0      1      0      1      0      1      0\n# ℹ 9 more variables: `2010` &lt;int&gt;, `2012` &lt;int&gt;, `2014` &lt;int&gt;, `2001` &lt;int&gt;,\n#   `2004` &lt;int&gt;, `2005` &lt;int&gt;, `2013` &lt;int&gt;, `2015` &lt;int&gt;, total &lt;dbl&gt;\n\n# c) Guest group trends over time\nplot_data &lt;- daily %&gt;%\n  mutate(\n    broad_group = case_when(\n      group %in% c(\"Acting\",\"Athletics\",\"Comedy\",\"Musician\") ~ \"Acting, Comedy & Music\",\n      group %in% c(\"Media\",\"media\",\"Science\",\"Academic\",\"Consultant\",\"Clergy\") ~ \"Media\",\n      group %in% c(\"Politician\",\"Political Aide\",\"Government\",\"Military\",\"Business\",\"Advocacy\") ~ \"Government & Politics\",\n      TRUE ~ NA_character_\n    )\n  ) %&gt;%\n  filter(!is.na(broad_group))                # drop uncategorized\n\nfraction_data &lt;- plot_data %&gt;%\n  count(year, broad_group) %&gt;%               # count per year × broad_group\n  group_by(year) %&gt;%\n  mutate(fraction = n / sum(n))              # fraction within each year\n\nggplot(fraction_data, aes(x = year, y = fraction, color = broad_group)) +\n  geom_line() +\n  labs(\n    x     = \"Year\",\n    y     = \"Fraction of Guests\",\n    color = \"Guest Category\",\n    title = \"Yearly Composition of The Daily Show Guests\"\n  )",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Reshaping</span>"
    ]
  },
  {
    "objectID": "ica/ica-joining.html",
    "href": "ica/ica-joining.html",
    "title": "\n14  Joining\n",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n# Exercise 1: Where are my keys?\n\n# Part a: mismatched key names\nstudents_2 &lt;- data.frame(\n  student = c(\"D\", \"E\", \"F\"),\n  class   = c(\"COMP 101\", \"BIOL 101\", \"POLI 101\")\n)\nenrollments_2 &lt;- data.frame(\n  course     = c(\"ART 101\", \"BIOL 101\", \"COMP 101\"),\n  enrollment = c(18, 20, 19)\n)\n\n# correct left join by specifying key columns\nstudents_2 %&gt;%\n  left_join(enrollments_2, join_by(class == course))\n\n  student    class enrollment\n1       D COMP 101         19\n2       E BIOL 101         20\n3       F POLI 101         NA\n\n# result: D→19, E→20, F→NA\n\n# Part c/d: joining on duplicate-named columns class & grade\nstudents_3 &lt;- data.frame(\n  student = c(\"Y\",\"Y\",\"Z\",\"Z\"),\n  class   = c(\"COMP 101\",\"BIOL 101\",\"POLI 101\",\"COMP 101\"),\n  grade   = c(\"B\",\"S\",\"C\",\"A\")\n)\nenrollments_3 &lt;- data.frame(\n  class      = c(\"ART 101\",\"BIOL 101\",\"COMP 101\"),\n  grade      = c(\"B\",\"A\",\"A-\"),\n  enrollment = c(20,18,19)\n)\n\n# only join on class, not grade\nstudents_3 %&gt;%\n  left_join(enrollments_3, join_by(class == class))\n\n  student    class grade.x grade.y enrollment\n1       Y COMP 101       B      A-         19\n2       Y BIOL 101       S       A         18\n3       Z POLI 101       C    &lt;NA&gt;         NA\n4       Z COMP 101       A      A-         19\n\n# grade.x = student grade; grade.y = average course grade\n\n# Exercise 2: More small practice\n\nvoters &lt;- data.frame(\n  id          = c(\"A\",\"D\",\"E\",\"F\",\"G\"),\n  times_voted = c(2,4,17,6,20)\n)\ncontact &lt;- data.frame(\n  name    = c(\"A\",\"B\",\"C\",\"D\"),\n  address = c(\"summit\",\"grand\",\"snelling\",\"fairview\"),\n  age     = c(24,89,43,38)\n)\n\n# 1. contact info for people who HAVEN'T voted\nanti_join(contact, voters, by = c(\"name\" = \"id\"))\n\n  name  address age\n1    B    grand  89\n2    C snelling  43\n\n# 2. contact info for people who HAVE voted\nsemi_join(contact, voters, by = c(\"name\" = \"id\"))\n\n  name  address age\n1    A   summit  24\n2    D fairview  38\n\n# 3. any data available on each person\nfull_join(voters, contact, by = c(\"id\" = \"name\"))\n\n  id times_voted  address age\n1  A           2   summit  24\n2  D           4 fairview  38\n3  E          17     &lt;NA&gt;  NA\n4  F           6     &lt;NA&gt;  NA\n5  G          20     &lt;NA&gt;  NA\n6  B          NA    grand  89\n7  C          NA snelling  43\n\n# 4. add contact info to the voting roster where possible\nleft_join(voters, contact, by = c(\"id\" = \"name\"))\n\n  id times_voted  address age\n1  A           2   summit  24\n2  D           4 fairview  38\n3  E          17     &lt;NA&gt;  NA\n4  F           6     &lt;NA&gt;  NA\n5  G          20     &lt;NA&gt;  NA\n\n# Exercise 3: Bigger datasets\n\ngrades &lt;- read.csv(\"https://mac-stat.github.io/data/grades.csv\") %&gt;%\n  distinct(sid, sessionID, .keep_all = TRUE)\ncourses &lt;- read.csv(\"https://mac-stat.github.io/data/courses.csv\")\n\n# how many rows and columns?\ndim(grades)   # grades: rows × cols\n\n[1] 5844    3\n\ndim(courses)  # courses: rows × cols\n\n[1] 1718    6\n\n# Exercise 4: Class size\n\n# Part a: combine cross-listed sessions by summing enrollments\ncourses_combined &lt;- courses %&gt;%\n  group_by(sessionID) %&gt;%\n  summarize(enroll = sum(enroll), .groups = \"drop\")\ndim(courses_combined)  # should be 1695 × 2\n\n[1] 1695    2\n\n# Part b: median class size overall\ncourses_combined %&gt;%\n  summarize(median_class_size = median(enroll))\n\n# A tibble: 1 × 1\n  median_class_size\n              &lt;int&gt;\n1                18\n\n# Part c: median class size experienced by each student\nstudent_class_size &lt;- grades %&gt;%\n  left_join(courses_combined, by = \"sessionID\") %&gt;%\n  group_by(sid) %&gt;%\n  summarize(median_size = median(enroll), .groups = \"drop\")\nhead(student_class_size)\n\n# A tibble: 6 × 2\n  sid    median_size\n  &lt;chr&gt;        &lt;dbl&gt;\n1 S31185        23.5\n2 S31188        21  \n3 S31191        25  \n4 S31194        15  \n5 S31197        24  \n6 S31200        21  \n\n# Part d: histogram of median class sizes per student\nggplot(student_class_size, aes(x = median_size)) +\n  geom_histogram() +\n  labs(x = \"Median Class Size\", y = \"Number of Students\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n# Exercise 5: Narrowing in on classes\n\n# Part a: students in session1986\ngrades %&gt;%\n  filter(sessionID == \"session1986\")\n\n     sid grade   sessionID\n1 S31401    B+ session1986\n2 S32247     B session1986\n\n# Part b: students in department E courses\ndept_E &lt;- courses %&gt;% filter(dept == \"E\")\ngrades %&gt;%\n  semi_join(dept_E, by = \"sessionID\") %&gt;%\n  distinct(sid)\n\n     sid\n1 S31245\n2 S31470\n3 S31938\n4 S31968\n5 S32022\n6 S32046\n7 S32226\n8 S32415\n9 S32484\n\n# Exercise 6: All the wrangling\n\n# GPA conversion table\ngpa_conversion &lt;- tibble(\n  grade = c(\"A+\",\"A\",\"A-\",\"B+\",\"B\",\"B-\",\"C+\",\"C\",\"C-\",\"D+\",\"D\",\"D-\",\"NC\",\"AU\",\"S\"),\n  gp    = c(4.3,4.0,3.7,3.3,3.0,2.7,2.3,2.0,1.7,1.3,1.0,0.7,0.0,NA,NA)\n)\n\n# Part a: total student enrollments by department\ngrades %&gt;%\n  left_join(courses_combined, by = \"sessionID\") %&gt;%\n  left_join(courses %&gt;% select(sessionID, dept), by = \"sessionID\") %&gt;%\n  count(dept, name = \"total_enrollments\") %&gt;%\n  arrange(desc(total_enrollments))\n\nWarning in left_join(., courses %&gt;% select(sessionID, dept), by = \"sessionID\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 64 of `x` matches multiple rows in `y`.\nℹ Row 807 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n   dept total_enrollments\n1     d               483\n2     M               410\n3     m               363\n4     O               359\n5     W               336\n6     q               318\n7     F               296\n8     k               265\n9     j               249\n10    D               240\n11    C               237\n12    G               237\n13    R               195\n14    n               191\n15    i               177\n16    Q               157\n17    J               148\n18    X               145\n19    p               129\n20    e               128\n21    K               112\n22    H               110\n23    N                99\n24    S                97\n25    b                67\n26    T                62\n27    Y                57\n28    t                56\n29    L                50\n30    V                50\n31    g                34\n32    s                31\n33    o                27\n34    I                26\n35    P                26\n36    B                24\n37    U                24\n38    E                12\n39    A                 2\n40    l                 1\n\n# Part b: GPA for each student\nstudent_gpa &lt;- grades %&gt;%\n  left_join(gpa_conversion, by = \"grade\") %&gt;%\n  group_by(sid) %&gt;%\n  summarize(gpa = mean(gp, na.rm = TRUE), .groups = \"drop\")\nhead(student_gpa)\n\n# A tibble: 6 × 2\n  sid      gpa\n  &lt;chr&gt;  &lt;dbl&gt;\n1 S31185  2.41\n2 S31188  3.02\n3 S31191  3.21\n4 S31194  3.36\n5 S31197  3.35\n6 S31200  2.2 \n\n# Part c: median GPA across all students\nstudent_gpa %&gt;%\n  summarize(median_gpa = median(gpa, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  median_gpa\n       &lt;dbl&gt;\n1       3.47\n\n# Part d: fraction of grades below B+ (gp &lt; 3.3)\ngrades %&gt;%\n  left_join(gpa_conversion, by = \"grade\") %&gt;%\n  summarize(fraction_below_Bplus = mean(gp &lt; 3.3, na.rm = TRUE))\n\n  fraction_below_Bplus\n1            0.2834776\n\n# Part e: GPA for each instructor\ngrades %&gt;%\n  left_join(courses, by = \"sessionID\") %&gt;%\n  left_join(gpa_conversion, by = \"grade\") %&gt;%\n  group_by(iid) %&gt;%\n  summarize(instructor_gpa = mean(gp, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  arrange(instructor_gpa)\n\nWarning in left_join(., courses, by = \"sessionID\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 64 of `x` matches multiple rows in `y`.\nℹ Row 807 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n# A tibble: 364 × 2\n   iid     instructor_gpa\n   &lt;chr&gt;            &lt;dbl&gt;\n 1 inst265           1.3 \n 2 inst444           1.7 \n 3 inst513           1.85\n 4 inst200           2   \n 5 inst507           2.2 \n 6 inst445           2.3 \n 7 inst420           2.6 \n 8 inst262           2.65\n 9 inst176           2.66\n10 inst234           2.7 \n# ℹ 354 more rows",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Joining</span>"
    ]
  },
  {
    "objectID": "ica/ica-factors.html",
    "href": "ica/ica-factors.html",
    "title": "\n15  Factors\n",
    "section": "",
    "text": "# Load tidyverse for dplyr, ggplot2, and forcats\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Reload and dedupe the grades data\ngrades &lt;- read.csv(\"https://mac-stat.github.io/data/grades.csv\") %&gt;%\n  distinct(sid, sessionID, .keep_all = TRUE)\n\n# Compute how many times each grade was assigned\ngrade_distribution &lt;- grades %&gt;% count(grade)\n\n\n# Exercise 1: Changing Order\n\n# a) Manually specify grade levels from highest to lowest, with 'S' and 'AU' last\ngrade_distribution %&gt;%\n  mutate(\n    grade = factor(\n      grade,\n      levels = c(\"A\",\"A-\",\"B+\",\"B\",\"B-\",\"C+\",\"C\",\"C-\",\"D+\",\"D\",\"D-\",\"NC\",\"S\",\"AU\")\n    )\n  ) %&gt;%\n  ggplot(aes(x = grade, y = n)) +\n    geom_col() +\n    labs(\n      title = \"Grade Distribution (Manual Order)\",\n      x = \"Grade\",\n      y = \"Count\"\n    )\n\n\n\n\n\n\n# • factor(..., levels=...) sets the exact ordering of grade categories.\n\n# b) Reorder grades by ascending frequency\ngrade_distribution %&gt;%\n  mutate(grade = fct_reorder(grade, n)) %&gt;%\n  ggplot(aes(x = grade, y = n)) +\n    geom_col() +\n    labs(\n      title = \"Grade Distribution (Ascending Frequency)\",\n      x = \"Grade (Least to Most Frequent)\",\n      y = \"Count\"\n    )\n\n\n\n\n\n\n# • fct_reorder(grade, n) reorders factor levels by the numeric 'n' in ascending order.\n\n# c) Reorder grades by descending frequency\ngrade_distribution %&gt;%\n  mutate(grade = fct_reorder(grade, n, .desc = TRUE)) %&gt;%\n  ggplot(aes(x = grade, y = n)) +\n    geom_col() +\n    labs(\n      title = \"Grade Distribution (Descending Frequency)\",\n      x = \"Grade (Most to Least Frequent)\",\n      y = \"Count\"\n    )\n\n\n\n\n\n\n# • fct_reorder(..., .desc=TRUE) flips the order to descending.\n\n# Exercise 2: Changing Factor Level Labels\n\ngrade_distribution %&gt;%\n  # First ensure the manual order\n  mutate(\n    grade = factor(\n      grade,\n      levels = c(\"A\",\"A-\",\"B+\",\"B\",\"B-\",\"C+\",\"C\",\"C-\",\"D+\",\"D\",\"D-\",\"NC\",\"S\",\"AU\")\n    )\n  ) %&gt;%\n  # Then recode two levels for clarity\n  mutate(\n    grade = fct_recode(\n      grade,\n      Audit        = \"AU\",\n      Satisfactory = \"S\"\n    )\n  ) %&gt;%\n  ggplot(aes(x = grade, y = n)) +\n    geom_col() +\n    labs(\n      title = \"Grade Distribution with Clear Labels\",\n      x = \"Grade\",\n      y = \"Count\"\n    )\n\n\n\n\n\n\n# • fct_recode(...) renames selected factor levels, leaving others unchanged.",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "ica/ica-strings.html",
    "href": "ica/ica-strings.html",
    "title": "\n16  Strings\n",
    "section": "",
    "text": "# Load required packages\nlibrary(tidyverse)  \n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)    \n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(rnaturalearth) \nlibrary(sf)          \n\nLinking to GEOS 3.13.0, GDAL 3.8.5, PROJ 9.5.1; sf_use_s2() is TRUE\n\nlibrary(maps)    \n\n\nAttaching package: 'maps'\n\nThe following object is masked from 'package:purrr':\n\n    map\n\nlibrary(RColorBrewer) \nlibrary(leaflet)    \nlibrary(fivethirtyeight)  \ncourses &lt;- read.csv(\"https://mac-stat.github.io/data/registrar.csv\")\n# Check it out\nhead(courses)\n\n       number   crn                                                name  days\n1 AMST 112-01 10318         Introduction to African American Literature M W F\n2 AMST 194-01 10073              Introduction to Asian American Studies M W F\n3 AMST 194-F1 10072 What’s After White Empire - And Is It Already Here?  T R \n4 AMST 203-01 10646 Politics and Inequality: The American Welfare State M W F\n5 AMST 205-01 10842                         Trans Theories and Politics  T R \n6 AMST 209-01 10474                   Civil Rights in the United States   W  \n             time      room             instructor avail_max\n1 9:40 - 10:40 am  MAIN 009       Daylanne English    3 / 20\n2  1:10 - 2:10 pm MUSIC 219          Jake Nagasawa   -4 / 16\n3  3:00 - 4:30 pm   HUM 214 Karin Aguilar-San Juan    0 / 14\n4 9:40 - 10:40 am  CARN 305          Lesley Lavery    3 / 25\n5  3:00 - 4:30 pm  MAIN 009              Myrl Beam   -2 / 20\n6 7:00 - 10:00 pm  MAIN 010         Walter Greason   -1 / 15\n\n\n\n# Exercise 1: Time slots\n# Count how many classes meet in each days × time slot, then show the top 6\ncourses %&gt;%\n  count(days, time, sort = TRUE) %&gt;%\n  head(6)\n\n   days             time  n\n1 M W F 10:50 - 11:50 am 76\n2  T R   9:40 - 11:10 am 71\n3 M W F  9:40 - 10:40 am 68\n4 M W F   1:10 - 2:10 pm 66\n5  T R    3:00 - 4:30 pm 62\n6  T R    1:20 - 2:50 pm 59\n\n# • ‘count(days, time)’ tallies each unique combination of meeting days and time\n# • ‘sort = TRUE’ orders by descending frequency\n\n# Exercise 2: Prep the data\ncourses_clean &lt;- courses %&gt;%\n  # split \"avail_max\" into two numeric columns\n  separate(avail_max, into = c(\"avail\", \"max\"), sep = \" / \") %&gt;%\n  mutate(across(c(avail, max), as.numeric),\n         enroll = max - avail) %&gt;%        # compute actual enrollment\n  # split \"number\" like \"AMST 112-01\" into dept, course number, section\n  separate(number,\n           into = c(\"dept\", \"number\", \"section\"),\n           sep = \"[- ]\",\n           extra = \"merge\") %&gt;%\n  relocate(dept, number, section, .before = name)\nhead(courses_clean)\n\n    crn dept number section                                                name\n1 10318 AMST    112      01         Introduction to African American Literature\n2 10073 AMST    194      01              Introduction to Asian American Studies\n3 10072 AMST    194      F1 What’s After White Empire - And Is It Already Here?\n4 10646 AMST    203      01 Politics and Inequality: The American Welfare State\n5 10842 AMST    205      01                         Trans Theories and Politics\n6 10474 AMST    209      01                   Civil Rights in the United States\n   days            time      room             instructor avail max enroll\n1 M W F 9:40 - 10:40 am  MAIN 009       Daylanne English     3  20     17\n2 M W F  1:10 - 2:10 pm MUSIC 219          Jake Nagasawa    -4  16     20\n3  T R   3:00 - 4:30 pm   HUM 214 Karin Aguilar-San Juan     0  14     14\n4 M W F 9:40 - 10:40 am  CARN 305          Lesley Lavery     3  25     22\n5  T R   3:00 - 4:30 pm  MAIN 009              Myrl Beam    -2  20     22\n6   W   7:00 - 10:00 pm  MAIN 010         Walter Greason    -1  15     16\n\n# • courses_clean now has: dept, number, section, name, days, time, room, instructor, avail, max, enroll\n\n# Exercise 3: Courses by department\n# a) 6 depts offering the most sections\ncourses_clean %&gt;%\n  count(dept, sort = TRUE) %&gt;%\n  head(6)\n\n  dept  n\n1 SPAN 45\n2 BIOL 44\n3 ENVI 38\n4 PSYC 37\n5 CHEM 33\n6 COMP 31\n\n# b) 6 depts with longest average course titles\ncourses_clean %&gt;%\n  mutate(title_length = nchar(name)) %&gt;%\n  group_by(dept) %&gt;%\n  summarize(avg_title_length = mean(title_length), .groups = \"drop\") %&gt;%\n  arrange(desc(avg_title_length)) %&gt;%\n  head(6)\n\n# A tibble: 6 × 2\n  dept  avg_title_length\n  &lt;chr&gt;            &lt;dbl&gt;\n1 WGSS              46.3\n2 INTL              41.4\n3 EDUC              39.4\n4 MCST              39.4\n5 POLI              37.4\n6 AMST              37.3\n\n# Exercise 4: STAT courses\n# a) Courses taught by Alicia Johnson\ncourses_clean %&gt;%\n  filter(instructor == \"Alicia Johnson\")\n\n    crn dept number section                         name  days            time\n1 10806 STAT    253      01 Statistical Machine Learning  T R  9:40 - 11:10 am\n2 10807 STAT    253      02 Statistical Machine Learning  T R   1:20 - 2:50 pm\n3 10808 STAT    253      03 Statistical Machine Learning  T R   3:00 - 4:30 pm\n        room     instructor avail max enroll\n1 THEATR 206 Alicia Johnson    -3  20     23\n2 THEATR 206 Alicia Johnson    -3  20     23\n3 THEATR 206 Alicia Johnson     2  20     18\n\n# b) STAT sections: clean up names, extract start time\nstat &lt;- courses_clean %&gt;%\n  filter(dept == \"STAT\") %&gt;%\n  mutate(\n    name      = str_remove(name, \"^Introduction to \"),\n    name      = str_replace(name, \"Statistical\", \"Stat\"),\n    start_time = str_trim(str_extract(time, \"^[^\\\\-]+\"))\n  ) %&gt;%\n  select(number, name, start_time, enroll)\nnrow(stat)  # should be 19\n\n[1] 19\n\nhead(stat)\n\n  number          name start_time enroll\n1    112  Data Science       3:00     27\n2    112  Data Science       9:40     21\n3    112  Data Science       1:20     25\n4    125  Epidemiology      12:00     26\n5    155 Stat Modeling       1:10     32\n6    155 Stat Modeling       9:40     24\n\n# Exercise 5: More cleaning for enrollments\nenrollments &lt;- courses_clean %&gt;%\n  filter(!dept %in% c(\"PE\", \"INTD\")) %&gt;%                             # drop PE and INTD\n  filter(!(dept == \"MUSI\" & as.numeric(number) &lt; 100),\n         !(dept == \"THDA\" & as.numeric(number) &lt; 100)) %&gt;%           # drop ensembles/practicums\n  filter(!str_detect(name, \"\\\\bLab\\\\b\"))                             # drop true lab sections\nhead(enrollments)\n\n    crn dept number section                                                name\n1 10318 AMST    112      01         Introduction to African American Literature\n2 10073 AMST    194      01              Introduction to Asian American Studies\n3 10072 AMST    194      F1 What’s After White Empire - And Is It Already Here?\n4 10646 AMST    203      01 Politics and Inequality: The American Welfare State\n5 10842 AMST    205      01                         Trans Theories and Politics\n6 10474 AMST    209      01                   Civil Rights in the United States\n   days            time      room             instructor avail max enroll\n1 M W F 9:40 - 10:40 am  MAIN 009       Daylanne English     3  20     17\n2 M W F  1:10 - 2:10 pm MUSIC 219          Jake Nagasawa    -4  16     20\n3  T R   3:00 - 4:30 pm   HUM 214 Karin Aguilar-San Juan     0  14     14\n4 M W F 9:40 - 10:40 am  CARN 305          Lesley Lavery     3  25     22\n5  T R   3:00 - 4:30 pm  MAIN 009              Myrl Beam    -2  20     22\n6   W   7:00 - 10:00 pm  MAIN 010         Walter Greason    -1  15     16\n\n# Exercise 6: Enrollment & departments\n# Example explorations:\n\n# Numerical summary: median and count of enrollments by department\nenrollments %&gt;%\n  group_by(dept) %&gt;%\n  summarize(\n    median_enroll = median(enroll),\n    section_count = n(),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(desc(section_count))\n\n# A tibble: 38 × 3\n   dept  median_enroll section_count\n   &lt;chr&gt;         &lt;dbl&gt;         &lt;int&gt;\n 1 ENGL           15.5            30\n 2 COMP           17              28\n 3 ENVI           18              28\n 4 ECON           20              27\n 5 POLI           18              27\n 6 PSYC           21              27\n 7 SPAN           15              26\n 8 BIOL           16              23\n 9 AMST           15              22\n10 HIST           15.5            22\n# ℹ 28 more rows\n\n# Visualization: distribution of section sizes by department\nggplot(enrollments, aes(x = enroll)) +\n  geom_histogram(binwidth = 5) +\n  facet_wrap(~ dept, scales = \"free_y\") +\n  labs(\n    x = \"Enrollment per Section\",\n    y = \"Number of Sections\",\n    title = \"Section Size Distributions by Department\"\n  )",
    "crumbs": [
      "In-class Activities",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Strings</span>"
    ]
  }
]